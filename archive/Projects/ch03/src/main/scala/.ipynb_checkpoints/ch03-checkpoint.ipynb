{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.immutable.Map\n",
       "import scala.collection.mutable.ArrayBuffer\n",
       "import scala.util.Random\n",
       "import org.apache.spark.broadcast.Broadcast\n",
       "import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n",
       "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.immutable.Map\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import scala.util.Random\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object RunRecommender\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object RunRecommender {\n",
    "    def main(args: Array[String]): Unit = {\n",
    "        val spark = SparkSession.builder().getOrCreate()\n",
    "        spark.sparkContext.setCheckpointDir(\"hdfs://soda-lab-bigdata-01:8020/tmp/\")\n",
    "        import spark.implicits._\n",
    "        val base = \"hdfs://soda-lab-bigdata-01:8020/dataset/chapter3/\"\n",
    "        val rawUserArtistData = spark.read.textFile(base + \"user_artist_data.txt\")\n",
    "        val rawArtistData = spark.read.textFile(base + \"artist_data.txt\")\n",
    "        val rawArtistAlias = spark.read.textFile(base + \"artist_alias.txt\")\n",
    "        \n",
    "        //Dataframe\n",
    "        val userArtistDF = rawUserArtistData.map{line => \n",
    "            val Array(user,artist,_*)=line.split(' ')\n",
    "            (user.toInt,artist.toInt)}.toDF(\"user\",\"artist\")\n",
    "        val artistByID = rawArtistData.flatMap{line => \n",
    "            val (id,name) = line.span(_!='\\t')\n",
    "            if (name.isEmpty){\n",
    "                None} \n",
    "            else {\n",
    "            try{\n",
    "                Some((id.toInt,name.trim))\n",
    "            } catch {\n",
    "                case _: NumberFormatException => None\n",
    "            }\n",
    "            }\n",
    "        }toDF(\"id\",\"name\")\n",
    "        val artistAlias = rawArtistAlias.flatMap { line =>\n",
    "          val Array(artist, alias) = line.split('\\t')\n",
    "          if (artist.isEmpty) {\n",
    "            None\n",
    "          } else {\n",
    "            Some((artist.toInt, alias.toInt))\n",
    "          }\n",
    "        }.collect().toMap\n",
    "        \n",
    "        //modeling prepare\n",
    "        def buildCounts(\n",
    "            rawUserArtistData: Dataset[String],\n",
    "            bArtistAlias: Broadcast[Map[Int,Int]]): DataFrame = {\n",
    "            rawUserArtistData.map{line =>\n",
    "                val Array(userID,artistID,count) = line.split(\" \").map(_.toInt)\n",
    "                val finalArtistID = \n",
    "                        bArtistAlias.value.getOrElse(artistID,artistID)\n",
    "                (userID, finalArtistID ,count)\n",
    "            }.toDF(\"user\",\"artist\",\"count\")\n",
    "        }\n",
    "        \n",
    "        val bArtistAlias = spark.sparkContext.broadcast(artistAlias)\n",
    "        val trainData = buildCounts(rawUserArtistData,bArtistAlias) \n",
    "        trainData.coalesce(1).write.csv(\"/dataset/chapter3/trainData.csv\")\n",
    "//         trainData.rdd.coalesce(1).saveAsTextFile(\"/dataset/chapter3/trainData.txt\")\n",
    "//         trainData.cache()\n",
    "        \n",
    "//         //modeling\n",
    "//         val model = new ALS().\n",
    "//             setSeed(Random.nextLong()).\n",
    "//             setImplicitPrefs(true).\n",
    "//             setRank(10).\n",
    "//             setRegParam(0.01).\n",
    "//             setAlpha(1.0).\n",
    "//             setMaxIter(5).\n",
    "//             setUserCol(\"user\").\n",
    "//             setItemCol(\"artist\").\n",
    "//             setRatingCol(\"count\").\n",
    "//             setPredictionCol(\"prediction\").\n",
    "//             fit(trainData)\n",
    "        \n",
    "//         val userID = 2093760\n",
    "\n",
    "//         val existingArtistIDs = trainData.\n",
    "//           filter($\"user\" === userID). \n",
    "//           select(\"artist\").as[Int].collect() \n",
    "\n",
    "//         artistByID.filter($\"id\" isin (existingArtistIDs:_*)).show() \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
