{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.16.8.92:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1584605995158)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@1cb187c\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datapath: String = /Users/soda/Downloads/backUp/Notebooks/AdvancedAnalyticswithSpark/data/\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datapath = \"/Users/soda/Downloads/backUp/Notebooks/AdvancedAnalyticswithSpark/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataWithoutHeader: org.apache.spark.sql.DataFrame = [_c0: int, _c1: string ... 40 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataWithoutHeader = spark.read.\n",
    "    option(\"inferSchema\",true).\n",
    "    option(\"header\",false).\n",
    "    csv(datapath+\"kddcup.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [duration: int, protocol_type: string ... 40 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = dataWithoutHeader.toDF(\n",
    "  \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "  \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "  \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "  \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "  \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "  \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "  \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "  \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "  \"dst_host_count\", \"dst_host_srv_count\",\n",
    "  \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "  \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "  \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "  \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "  \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42,4898431)\n"
     ]
    }
   ],
   "source": [
    "println(data.columns.length,data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|           label|  count|\n",
      "+----------------+-------+\n",
      "|          smurf.|2807886|\n",
      "|        neptune.|1072017|\n",
      "|         normal.| 972781|\n",
      "|          satan.|  15892|\n",
      "|        ipsweep.|  12481|\n",
      "|      portsweep.|  10413|\n",
      "|           nmap.|   2316|\n",
      "|           back.|   2203|\n",
      "|    warezclient.|   1020|\n",
      "|       teardrop.|    979|\n",
      "|            pod.|    264|\n",
      "|   guess_passwd.|     53|\n",
      "|buffer_overflow.|     30|\n",
      "|           land.|     21|\n",
      "|    warezmaster.|     20|\n",
      "|           imap.|     12|\n",
      "|        rootkit.|     10|\n",
      "|     loadmodule.|      9|\n",
      "|      ftp_write.|      8|\n",
      "|       multihop.|      7|\n",
      "|            phf.|      4|\n",
      "|           perl.|      3|\n",
      "|            spy.|      2|\n",
      "+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"label\").groupBy(\"label\").count().orderBy($\"count\".desc).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [duration: int, protocol_type: string ... 40 more fields]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [duration: int, protocol_type: string ... 40 more fields]\n"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainData, testData) = data.randomSplit(Array(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataSample: org.apache.spark.sql.DataFrame = [duration: int, protocol_type: string ... 40 more fields]\n"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataSample = testData.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|           label| count|\n",
      "+----------------+------+\n",
      "|          smurf.|561923|\n",
      "|        neptune.|214735|\n",
      "|         normal.|194824|\n",
      "|          satan.|  3238|\n",
      "|        ipsweep.|  2590|\n",
      "|      portsweep.|  2140|\n",
      "|           nmap.|   454|\n",
      "|           back.|   413|\n",
      "|       teardrop.|   193|\n",
      "|    warezclient.|   186|\n",
      "|            pod.|    53|\n",
      "|   guess_passwd.|    12|\n",
      "|buffer_overflow.|    10|\n",
      "|    warezmaster.|     3|\n",
      "|        rootkit.|     2|\n",
      "|     loadmodule.|     2|\n",
      "|           land.|     2|\n",
      "|           imap.|     1|\n",
      "|           perl.|     1|\n",
      "|      ftp_write.|     1|\n",
      "|            phf.|     1|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// dataSample.select(\"label\").groupBy(\"label\").count().orderBy($\"count\".desc).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.sql.Row = [0,tcp,http,SF,215,45076,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,normal.]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.clustering.{KMeans, KMeansModel}\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "numericOnly: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [duration: int, src_bytes: int ... 37 more fields]\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_56c71104e9a0\n",
       "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_641090b27e21\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_33d94be62a20\n",
       "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_33d94be62a20\n",
       "kmeansModel: org.apache.spark.ml.clustering.KMeansModel = kmeans_641090b27e21\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.clustering.{KMeans,KMeansModel}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val numericOnly = dataSample.drop(\"protocol_type\",\"service\",\"flag\").cache()\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(numericOnly.columns.filter(_!=\"label\")).\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "val kmeans = new KMeans().\n",
    "    setPredictionCol(\"cluster\").\n",
    "    setFeaturesCol(\"featureVector\")\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler,kmeans))\n",
    "val pipelineModel = pipeline.fit(numericOnly)\n",
    "val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49.38001168454184,1501.835438624038,1049.7732933788616,2.0391870576875822E-6,6.372459555273694E-4,2.1411464105719613E-5,0.0118068930640111,3.2626992923001316E-5,0.14353022024239817,0.007641853498684214,7.137154701906538E-5,3.670536703837648E-5,0.012652136099422603,0.0011541798746511716,7.544992113444054E-5,0.0010175543417861035,0.0,1.0195935288437911E-6,8.014005136712198E-4,334.8298991723959,295.0964443714869,0.17803137901044383,0.17809188169044526,0.05756938079065412,0.057643026031242356,0.7897316939628652,0.02127689815178726,0.02838519835682347,232.98639658313817,189.12281106014277,0.7532675627534575,0.03089434665976848,0.6050103437762813,0.006458625404396378,0.17817002333849663,0.17795975256504307,0.057846822385788074,0.057605433617834406]\n",
      "[36071.0,1.379963888E9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,0.0,0.0,77.0,1.0,0.01,0.12,0.06,0.0,0.0,0.0,0.06,1.0]\n"
     ]
    }
   ],
   "source": [
    "kmeansModel.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "withCluster: org.apache.spark.sql.DataFrame = [duration: int, src_bytes: int ... 39 more fields]\n"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withCluster = pipelineModel.transform(numericOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------+\n",
      "|cluster|           label| count|\n",
      "+-------+----------------+------+\n",
      "|      0|          smurf.|561923|\n",
      "|      0|        neptune.|214735|\n",
      "|      0|         normal.|194824|\n",
      "|      0|          satan.|  3238|\n",
      "|      0|        ipsweep.|  2590|\n",
      "|      0|      portsweep.|  2139|\n",
      "|      0|           nmap.|   454|\n",
      "|      0|           back.|   413|\n",
      "|      0|       teardrop.|   193|\n",
      "|      0|    warezclient.|   186|\n",
      "|      0|            pod.|    53|\n",
      "|      0|   guess_passwd.|    12|\n",
      "|      0|buffer_overflow.|    10|\n",
      "|      0|    warezmaster.|     3|\n",
      "|      0|           land.|     2|\n",
      "|      0|        rootkit.|     2|\n",
      "|      0|     loadmodule.|     2|\n",
      "|      0|            phf.|     1|\n",
      "|      0|           perl.|     1|\n",
      "|      0|           imap.|     1|\n",
      "|      0|      ftp_write.|     1|\n",
      "|      1|      portsweep.|     1|\n",
      "+-------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "withCluster.select(\"cluster\",\"label\").\n",
    "    groupBy(\"cluster\",\"label\").count().\n",
    "    orderBy($\"cluster\",$\"count\".desc).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.Random\n",
       "import org.apache.spark.sql.DataFrame\n",
       "clusteringScore0: (data: org.apache.spark.sql.DataFrame, k: Int)Double\n"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Sum of Square\n",
    "import scala.util.Random\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def clusteringScore0(data:DataFrame,k:Int):Double = {\n",
    "    val assembler = new VectorAssembler().\n",
    "        setInputCols(data.columns.filter(_!=\"label\")).\n",
    "        setOutputCol(\"featureVector\")\n",
    "    val kmeans = new KMeans().\n",
    "        setSeed(Random.nextLong()).\n",
    "        setK(k).\n",
    "        setPredictionCol(\"cluster\").\n",
    "        setFeaturesCol(\"featureVector\")\n",
    "    val pipeline = new Pipeline().setStages(Array(assembler,kmeans))\n",
    "    val kmeansModel = pipeline.fit(data).stages.last.asInstanceOf[KMeansModel]\n",
    "    kmeansModel.computeCost(assembler.transform(data))/data.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,8341731.132478421)\n",
      "(40,7127933.177598517)\n",
      "(60,3542366.9420348885)\n",
      "(80,3434540.5904618637)\n",
      "(100,3400234.45255818)\n"
     ]
    }
   ],
   "source": [
    "(20 to 100 by 20).map(k => (k, clusteringScore0(numericOnly, k))).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## randomSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obs: org.apache.spark.sql.DataFrame = [_c0: double, _c1: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val obs = spark.read.option(\"inferSchema\",true).csv(\"/Users/soda/Downloads/iris.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_c0: double, _c1: double ... 3 more fields]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_c0: double, _c1: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainData, testData) = obs.randomSplit(Array(0.9, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     (count / 132)|\n",
      "+------------------+\n",
      "|0.3409090909090909|\n",
      "|0.3181818181818182|\n",
      "|0.3409090909090909|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainData.toDF().groupBy(\"_c4\").count().select($\"count\"/trainData.count()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----------+\n",
      "|_c0|_c1|_c2|_c3|        _c4|\n",
      "+---+---+---+---+-----------+\n",
      "|4.3|3.0|1.1|0.1|Iris-setosa|\n",
      "|4.4|2.9|1.4|0.2|Iris-setosa|\n",
      "|4.4|3.0|1.3|0.2|Iris-setosa|\n",
      "+---+---+---+---+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainData.toDF().show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sampleBy\n",
    "http://allaboutscala.com/big-data/spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fraction: scala.collection.immutable.Map[String,Double] = Map(Iris-virginica -> 0.1, Iris-setosa -> 0.1, Iris-versicolor -> 0.1)\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// https://spark.apache.org/docs/1.6.2/api/java/org/apache/spark/sql/DataFrameStatFunctions.html\n",
    "val fraction = Map(\"Iris-virginica\" -> 0.1,\n",
    "                  \"Iris-setosa\" -> 0.1,\n",
    "                  \"Iris-versicolor\" -> 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|            _c4|count|\n",
      "+---------------+-----+\n",
      "| Iris-virginica|    6|\n",
      "|    Iris-setosa|    6|\n",
      "|Iris-versicolor|    5|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs.stat.sampleBy(\"_c4\",fraction,15L).groupBy(\"_c4\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/41516805/how-to-select-a-same-size-stratified-sample-from-a-dataframe-in-apache-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- src_bytes: integer (nullable = true)\n",
      " |-- dst_bytes: integer (nullable = true)\n",
      " |-- land: integer (nullable = true)\n",
      " |-- wrong_fragment: integer (nullable = true)\n",
      " |-- urgent: integer (nullable = true)\n",
      " |-- hot: integer (nullable = true)\n",
      " |-- num_failed_logins: integer (nullable = true)\n",
      " |-- logged_in: integer (nullable = true)\n",
      " |-- num_compromised: integer (nullable = true)\n",
      " |-- root_shell: integer (nullable = true)\n",
      " |-- su_attempted: integer (nullable = true)\n",
      " |-- num_root: integer (nullable = true)\n",
      " |-- num_file_creations: integer (nullable = true)\n",
      " |-- num_shells: integer (nullable = true)\n",
      " |-- num_access_files: integer (nullable = true)\n",
      " |-- num_outbound_cmds: integer (nullable = true)\n",
      " |-- is_host_login: integer (nullable = true)\n",
      " |-- is_guest_login: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- srv_count: integer (nullable = true)\n",
      " |-- serror_rate: double (nullable = true)\n",
      " |-- srv_serror_rate: double (nullable = true)\n",
      " |-- rerror_rate: double (nullable = true)\n",
      " |-- srv_rerror_rate: double (nullable = true)\n",
      " |-- same_srv_rate: double (nullable = true)\n",
      " |-- diff_srv_rate: double (nullable = true)\n",
      " |-- srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_count: integer (nullable = true)\n",
      " |-- dst_host_srv_count: integer (nullable = true)\n",
      " |-- dst_host_same_srv_rate: double (nullable = true)\n",
      " |-- dst_host_diff_srv_rate: double (nullable = true)\n",
      " |-- dst_host_same_src_port_rate: double (nullable = true)\n",
      " |-- dst_host_srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_serror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_serror_rate: double (nullable = true)\n",
      " |-- dst_host_rerror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_rerror_rate: double (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.row_number\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "sampling: org.apache.spark.sql.DataFrame = [duration: int, protocol_type: string ... 41 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.row_number\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "\n",
    "val sampling = data.withColumn(\"row_num\",\n",
    "              row_number().over(Window.partitionBy($\"label\").orderBy($\"duration\"))).\n",
    "              where(col(\"row_num\") <= 5).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4898431,4898431)\n"
     ]
    }
   ],
   "source": [
    "println(sampling.count(),data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Long = 109\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.8　特征的规范化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StandardScaler\n",
       "clusteringScore2: (data: org.apache.spark.sql.DataFrame, k: Int)Double\n"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "def clusteringScore2(data: DataFrame, k: Int): Double = {\n",
    "  val assembler = new VectorAssembler().\n",
    "    setInputCols(data.columns.filter(_ != \"label\")).\n",
    "    setOutputCol(\"featureVector\")\n",
    "  val scaler = new StandardScaler()\n",
    "    .setInputCol(\"featureVector\")\n",
    "    .setOutputCol(\"scaledFeatureVector\")\n",
    "    .setWithStd(true)\n",
    "    .setWithMean(false)\n",
    "  val kmeans = new KMeans().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setK(k).\n",
    "    setPredictionCol(\"cluster\").\n",
    "    setFeaturesCol(\"scaledFeatureVector\").\n",
    "    setMaxIter(40).\n",
    "    setTol(1.0e-5)\n",
    "  val pipeline = new Pipeline().setStages(Array(assembler, scaler, kmeans))\n",
    "  val pipelineModel = pipeline.fit(data)\n",
    "  val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]\n",
    "  kmeansModel.computeCost(pipelineModel.transform(data)) / data.count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60,1.0645668998700453)\n",
      "(90,0.6114058873865597)\n",
      "(120,0.41312963763016175)\n",
      "(150,0.3260755679793301)\n",
      "(180,0.2655638216810723)\n"
     ]
    }
   ],
   "source": [
    "(60 to 180 by 30).map(k => (k,clusteringScore2(numericOnly,k))).\n",
    "    foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.9　类别型变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\n",
       "oneHotPipeline: (inputCol: String)(org.apache.spark.ml.Pipeline, String)\n"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{OneHotEncoder,StringIndexer}\n",
    "\n",
    "def oneHotPipeline(inputCol:String):(Pipeline,String) = {\n",
    "    val indexer = new StringIndexer().\n",
    "        setInputCol(inputCol).\n",
    "        setOutputCol(inputCol+\"_indexed\")\n",
    "    \n",
    "    val encoder = new OneHotEncoder().\n",
    "        setInputCol(inputCol+\"_indexed\").\n",
    "        setOutputCol(inputCol+\"_vec\")\n",
    "    \n",
    "    val pipeline = new Pipeline().setStages(Array(indexer,encoder))\n",
    "    (pipeline,inputCol+\"_vec\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entropy: (counts: Iterable[Int])Double\n"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entropy(counts: Iterable[Int]): Double = {\n",
    "  val values = counts.filter(_ > 0)\n",
    "  val n = values.map(_.toDouble).sum\n",
    "  values.map { v =>\n",
    "    val p = v / n\n",
    "    -p * math.log(p)\n",
    "  }.sum\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val clusterLabel = pipelineModel.transform(data).\n",
    "  select(\"cluster\", \"label\").as[(Int, String)] \n",
    "\n",
    "val weightedClusterEntropy = clusterLabel.\n",
    "  groupByKey { case (cluster, _) => cluster }. \n",
    "  mapGroups { case (_, clusterLabels) =>\n",
    "    val labels = clusterLabels.map { case (_, label) => label }.toSeq\n",
    "    val labelCounts = labels.groupBy(identity).values.map(_.size) \n",
    "    labels.size * entropy(labelCounts)\n",
    "  }.collect()\n",
    "\n",
    "weightedClusterEntropy.sum / data.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
