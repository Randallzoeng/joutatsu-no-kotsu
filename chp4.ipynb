{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.16.8.92:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1583741685962)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@2321e284\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = /Users/soda/Downloads/backUp/Notebooks/AdvancedAnalyticswithSpark/data/\n",
       "dataWithoutHeader: org.apache.spark.sql.DataFrame = [_c0: int, _c1: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"/Users/soda/Downloads/backUp/Notebooks/AdvancedAnalyticswithSpark/data/\"\n",
    "val dataWithoutHeader = spark.read.\n",
    "    option(\"inferSchema\",true).\n",
    "    option(\"header\",false).\n",
    "    csv(path+\"covtype.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colNames: Seq[String] = List(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, Soil_..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colNames = Seq(\n",
    "    \"Elevation\", \"Aspect\", \"Slope\",\n",
    "    \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\"\n",
    "  ) ++ ( \n",
    "    (0 until 4).map(i => s\"Wilderness_Area_$i\")\n",
    "  ) ++ (\n",
    "    (0 until 40).map(i => s\"Soil_Type_$i\")\n",
    "  ) ++ Seq(\"Cover_Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 在 Spark MLlib 所有 API 中，目标列通常都被视为双精度浮点数类型而不是整型\n",
    "val data = dataWithoutHeader.toDF(colNames:_*).\n",
    "    withColumn(\"Cover_Type\",$\"Cover_Type\".cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.sql.Row = [2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5.0]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7　第一棵决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\n",
       "res2: testData.type = [Elevation: int, Aspect: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainData,testData) = data.randomSplit(Array(0.9,0.1))\n",
    "trainData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark MLlib 要求将所有输入合并成一列，该列的值是一个向量。这个类是线性代数中向量的一个抽象，仅包含一些数字。对于大多数的意图和目的，向量就像是一个简单的双精度浮点数数组。当然，有些输入特征在概念上是类别型的，即使在输入中它们都是用数值表示的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "inputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_T..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val inputCols = trainData.columns.filter( _!=\"Cover_Type\")\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(inputCols).\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "val assemblerTrainData = assembler.transform(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|featureVector                                                                                        |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1863.0,37.0,17.0,120.0,18.0,90.0,217.0,202.0,115.0,769.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,5,6,7,8,9,13,18],[1874.0,18.0,14.0,90.0,208.0,209.0,135.0,793.0,1.0,1.0])                 |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1889.0,28.0,22.0,150.0,23.0,120.0,205.0,185.0,108.0,759.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1889.0,353.0,30.0,95.0,39.0,67.0,153.0,172.0,146.0,600.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1896.0,337.0,12.0,30.0,6.0,175.0,195.0,224.0,168.0,732.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1898.0,34.0,23.0,175.0,56.0,134.0,210.0,184.0,99.0,765.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1899.0,355.0,22.0,153.0,43.0,124.0,178.0,195.0,151.0,819.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1901.0,311.0,9.0,30.0,2.0,190.0,195.0,234.0,179.0,726.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1903.0,5.0,13.0,42.0,4.0,201.0,203.0,214.0,148.0,708.0,1.0,1.0])    |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1903.0,67.0,16.0,108.0,36.0,120.0,234.0,207.0,100.0,969.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1904.0,51.0,26.0,67.0,30.0,162.0,222.0,175.0,72.0,711.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1905.0,33.0,27.0,90.0,46.0,150.0,204.0,171.0,89.0,725.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1905.0,77.0,21.0,90.0,38.0,120.0,241.0,196.0,75.0,1025.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1906.0,356.0,20.0,150.0,55.0,120.0,184.0,201.0,151.0,726.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1908.0,323.0,32.0,150.0,52.0,120.0,125.0,190.0,196.0,765.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1916.0,24.0,25.0,212.0,74.0,175.0,197.0,177.0,105.0,789.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1916.0,320.0,24.0,190.0,60.0,162.0,151.0,210.0,195.0,832.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,23],[1918.0,321.0,28.0,42.0,17.0,85.0,139.0,201.0,196.0,402.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1919.0,30.0,22.0,67.0,9.0,256.0,208.0,188.0,107.0,661.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1919.0,44.0,26.0,162.0,68.0,150.0,216.0,173.0,77.0,706.0,1.0,1.0])  |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assemblerTrainData.select(\"featureVector\").show(truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
       "import scala.util.Random\n",
       "classifier: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_70fcec132ebe\n",
       "model: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel (uid=dtc_70fcec132ebe) of depth 5 with 47 nodes\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import scala.util.Random\n",
    "\n",
    "val classifier = new DecisionTreeClassifier().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setFeaturesCol(\"featureVector\").\n",
    "    setPredictionCol(\"prediction\")\n",
    "\n",
    "val model = classifier.fit(assemblerTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=dtc_70fcec132ebe) of depth 5 with 47 nodes\n",
      "  If (feature 0 <= 3047.5)\n",
      "   If (feature 0 <= 2514.5)\n",
      "    If (feature 10 <= 0.5)\n",
      "     If (feature 3 <= 15.0)\n",
      "      If (feature 12 <= 0.5)\n",
      "       Predict: 4.0\n",
      "      Else (feature 12 > 0.5)\n",
      "       Predict: 6.0\n",
      "     Else (feature 3 > 15.0)\n",
      "      Predict: 3.0\n",
      "    Else (feature 10 > 0.5)\n",
      "     If (feature 9 <= 5482.5)\n",
      "      Predict: 2.0\n",
      "     Else (feature 9 > 5482.5)\n",
      "      If (feature 5 <= 621.0)\n",
      "       Predict: 2.0\n",
      "      Else (feature 5 > 621.0)\n",
      "       Predict: 5.0\n",
      "   Else (feature 0 > 2514.5)\n",
      "    If (feature 17 <= 0.5)\n",
      "     If (feature 0 <= 2951.5)\n",
      "      If (feature 15 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 15 > 0.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 0 > 2951.5)\n",
      "      Predict: 2.0\n",
      "    Else (feature 17 > 0.5)\n",
      "     If (feature 0 <= 2708.5)\n",
      "      Predict: 3.0\n",
      "     Else (feature 0 > 2708.5)\n",
      "      If (feature 5 <= 1178.0)\n",
      "       Predict: 5.0\n",
      "      Else (feature 5 > 1178.0)\n",
      "       Predict: 2.0\n",
      "  Else (feature 0 > 3047.5)\n",
      "   If (feature 0 <= 3309.5)\n",
      "    If (feature 7 <= 239.5)\n",
      "     If (feature 0 <= 3132.5)\n",
      "      If (feature 45 <= 0.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 45 > 0.5)\n",
      "       Predict: 2.0\n",
      "     Else (feature 0 > 3132.5)\n",
      "      Predict: 1.0\n",
      "    Else (feature 7 > 239.5)\n",
      "     If (feature 3 <= 300.5)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 300.5)\n",
      "      If (feature 0 <= 3199.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 3199.5)\n",
      "       Predict: 1.0\n",
      "   Else (feature 0 > 3309.5)\n",
      "    If (feature 12 <= 0.5)\n",
      "     If (feature 3 <= 284.0)\n",
      "      If (feature 6 <= 205.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 6 > 205.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 3 > 284.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 12 > 0.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      Predict: 7.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 970.5)\n",
      "       Predict: 7.0\n",
      "      Else (feature 5 > 970.5)\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8340238829800503,Elevation)\n",
      "(0.032560153307420445,Soil_Type_3)\n",
      "(0.02992060724618838,Soil_Type_1)\n",
      "(0.02597536404540168,Hillshade_Noon)\n",
      "(0.02497609703087602,Soil_Type_31)\n",
      "(0.023110508973576863,Horizontal_Distance_To_Hydrology)\n",
      "(0.015196520961657254,Wilderness_Area_2)\n",
      "(0.00669560030172463,Wilderness_Area_0)\n",
      "(0.004565310672848323,Horizontal_Distance_To_Roadways)\n",
      "(0.0026181830974530335,Hillshade_9am)\n",
      "(3.577713828031636E-4,Horizontal_Distance_To_Fire_Points)\n",
      "(0.0,Wilderness_Area_3)\n",
      "(0.0,Wilderness_Area_1)\n",
      "(0.0,Vertical_Distance_To_Hydrology)\n",
      "(0.0,Soil_Type_9)\n",
      "(0.0,Soil_Type_8)\n",
      "(0.0,Soil_Type_7)\n",
      "(0.0,Soil_Type_6)\n",
      "(0.0,Soil_Type_5)\n",
      "(0.0,Soil_Type_4)\n",
      "(0.0,Soil_Type_39)\n",
      "(0.0,Soil_Type_38)\n",
      "(0.0,Soil_Type_37)\n",
      "(0.0,Soil_Type_36)\n",
      "(0.0,Soil_Type_35)\n",
      "(0.0,Soil_Type_34)\n",
      "(0.0,Soil_Type_33)\n",
      "(0.0,Soil_Type_32)\n",
      "(0.0,Soil_Type_30)\n",
      "(0.0,Soil_Type_29)\n",
      "(0.0,Soil_Type_28)\n",
      "(0.0,Soil_Type_27)\n",
      "(0.0,Soil_Type_26)\n",
      "(0.0,Soil_Type_25)\n",
      "(0.0,Soil_Type_24)\n",
      "(0.0,Soil_Type_23)\n",
      "(0.0,Soil_Type_22)\n",
      "(0.0,Soil_Type_21)\n",
      "(0.0,Soil_Type_20)\n",
      "(0.0,Soil_Type_2)\n",
      "(0.0,Soil_Type_19)\n",
      "(0.0,Soil_Type_18)\n",
      "(0.0,Soil_Type_17)\n",
      "(0.0,Soil_Type_16)\n",
      "(0.0,Soil_Type_15)\n",
      "(0.0,Soil_Type_14)\n",
      "(0.0,Soil_Type_13)\n",
      "(0.0,Soil_Type_12)\n",
      "(0.0,Soil_Type_11)\n",
      "(0.0,Soil_Type_10)\n",
      "(0.0,Soil_Type_0)\n",
      "(0.0,Slope)\n",
      "(0.0,Hillshade_3pm)\n",
      "(0.0,Aspect)\n"
     ]
    }
   ],
   "source": [
    "model.featureImportances.toArray.zip(inputCols).\n",
    "    sorted.reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 57 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(assemblerTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|Cover_Type|prediction|probability                                                                                                      |\n",
      "+----------+----------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|6.0       |3.0       |[0.0,2.794076557697681E-5,0.07066219614417435,0.6050013970382788,0.040765576976809166,0.0,0.2835428890751607,0.0]|\n",
      "|6.0       |4.0       |[0.0,0.0,0.06932668329177058,0.2937655860349127,0.5032418952618454,0.0,0.1336658354114713,0.0]                   |\n",
      "|6.0       |3.0       |[0.0,2.794076557697681E-5,0.07066219614417435,0.6050013970382788,0.040765576976809166,0.0,0.2835428890751607,0.0]|\n",
      "|6.0       |3.0       |[0.0,2.794076557697681E-5,0.07066219614417435,0.6050013970382788,0.040765576976809166,0.0,0.2835428890751607,0.0]|\n",
      "|6.0       |3.0       |[0.0,2.794076557697681E-5,0.07066219614417435,0.6050013970382788,0.040765576976809166,0.0,0.2835428890751607,0.0]|\n",
      "+----------+----------+-----------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"Cover_Type\",\"prediction\",\"probability\").show(5,truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_9ceaef7f005f\n",
       "res8: Double = 0.6887950370053781\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator().\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setPredictionCol(\"prediction\")\n",
    "\n",
    "evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "evaluator.setMetricName(\"f1\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "predictionRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[103] at rdd at <console>:35\n",
       "multiclassMetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@565847fd\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "\n",
    "val predictionRDD = predictions.\n",
    "    select(\"prediction\",\"Cover_Type\").\n",
    "    as[(Double,Double)].\n",
    "    rdd\n",
    "\n",
    "val multiclassMetrics = new MulticlassMetrics(predictionRDD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.mllib.linalg.Matrix =\n",
       "126481.0  58304.0   98.0     0.0     24.0   8.0    5670.0\n",
       "48460.0   201271.0  3831.0   139.0   298.0  65.0   845.0\n",
       "0.0       3581.0    27926.0  589.0   25.0   149.0  0.0\n",
       "0.0       3.0       1459.0   1009.0  0.0    0.0    0.0\n",
       "0.0       7834.0    282.0    0.0     430.0  0.0    0.0\n",
       "0.0       4058.0    10789.0  268.0   8.0    587.0  0.0\n",
       "7855.0    76.0      0.0      0.0     0.0    0.0    10570.0\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclassMetrics.confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confusionMatrix: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Cover_Type: double, 1: bigint ... 6 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val confusionMatrix = predictions.groupBy(\"Cover_Type\").\n",
    "    pivot(\"prediction\",(1 to 7)).\n",
    "    count().\n",
    "    na.fill(0.0).\n",
    "    orderBy(\"Cover_Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-----+----+---+---+-----+\n",
      "|Cover_Type|     1|     2|    3|   4|  5|  6|    7|\n",
      "+----------+------+------+-----+----+---+---+-----+\n",
      "|       1.0|126481| 58304|   98|   0| 24|  8| 5670|\n",
      "|       2.0| 48460|201271| 3831| 139|298| 65|  845|\n",
      "|       3.0|     0|  3581|27926| 589| 25|149|    0|\n",
      "|       4.0|     0|     3| 1459|1009|  0|  0|    0|\n",
      "|       5.0|     0|  7834|  282|   0|430|  0|    0|\n",
      "|       6.0|     0|  4058|10789| 268|  8|587|    0|\n",
      "|       7.0|  7855|    76|    0|   0|  0|  0|10570|\n",
      "+----------+------+------+-----+----+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusionMatrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "classProbabilities: (data: org.apache.spark.sql.DataFrame)Array[Double]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def classProbabilities(data: DataFrame): Array[Double] = {\n",
    "    val total = data.count()\n",
    "    data.groupBy(\"Cover_Type\").count().\n",
    "        orderBy(\"Cover_Type\").\n",
    "        select(\"count\").as[Double].\n",
    "        map(_ / total).\n",
    "        collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainPriorProb: Array[Double] = Array(0.36441283996695933, 0.4874051610732095, 0.06170266466791079, 0.004724737663291217, 0.016340594119986538, 0.03003870040077095, 0.03537530210787163)\n",
       "testPriorProb: Array[Double] = Array(0.36633919338159254, 0.4893485005170631, 0.06004825922095829, 0.004756980351602895, 0.016321957945536023, 0.028559117545673904, 0.03462599103757325)\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainPriorProb = classProbabilities(trainData)\n",
    "val testPriorProb = classProbabilities(testData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
