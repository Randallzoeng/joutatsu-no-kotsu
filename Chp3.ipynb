{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.oreilly.com/catalog/errata.csp?isbn=0636920035091\n",
    "https://github.com/sryza/aas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.16.8.92:4041\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1582769049048)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@79353f55\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%init_spark\n",
    "launcher.driver_memory = '4g'\n",
    "// http://localhost:8888/notebooks/Downloads/backUp/Notebooks/AdvancedAnalyticswithSpark/PracNotes/Ch3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datapath_user: String = /Users/soda/Downloads/dataSet/aas/profiledata_06-May-2005/user_artist_data.txt\n",
       "rawUserArtistData: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datapath_user = \"/Users/soda/Downloads/dataSet/aas/profiledata_06-May-2005/user_artist_data.txt\"\n",
    "val rawUserArtistData = spark.read.textFile(datapath_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000002 1 55\n",
      "1000002 1000006 33\n",
      "1000002 1000007 8\n",
      "1000002 1000009 144\n",
      "1000002 1000010 314\n"
     ]
    }
   ],
   "source": [
    "rawUserArtistData.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userArtistDF: org.apache.spark.sql.DataFrame = [user: int, artist: int]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val userArtistDF = rawUserArtistData.map{line => \n",
    "        val Array(user, artist, _*)=line.split(\" \")\n",
    "        (user.toInt,artist.toInt)\n",
    "}.toDF(\"user\",\"artist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark MLlib 的 ALS 算法实现并不严格要求用户和产品的 ID 必须是数值型，不过当 ID 为 32 位非负整数时，效率会更高。使用 Int 表示 ID 是有好处的，但同时意味着 ID 不能超过 Int 的最大值（Int.MaxValue），即 2147483647。我们的数据集是否已经满足了这个要求。\n",
    "最大的用户 ID 和艺术家 ID 分别是 2443548 和 10794401，而它们的最小值分别是 90 和 1，并没有出现负值。这些远比 2147483647 要小，所以在使用这些 ID 之前，没有必要进行额外的转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+-----------+\n",
      "|min(user)|max(user)|min(artist)|max(artist)|\n",
      "+---------+---------+-----------+-----------+\n",
      "|       90|  2443548|          1|   10794401|\n",
      "+---------+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userArtistDF.agg(\n",
    "min(\"user\"),max(\"user\"),min(\"artist\"),max(\"artist\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datapath_artist: String = /Users/soda/Downloads/dataSet/aas/profiledata_06-May-2005/artist_data.txt\n",
       "rawArtistData: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//建立映射\n",
    "val datapath_artist = \"/Users/soda/Downloads/dataSet/aas/profiledata_06-May-2005/artist_data.txt\"\n",
    "val rawArtistData = spark.read.textFile(datapath_artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1134999\t06Crazy Life\n",
      "6821360\tPang Nakarin\n",
      "10113088\tTerfel, Bartoli- Mozart: Don\n",
      "10151459\tThe Flaming Sidebur\n",
      "6826647\tBodenstandig 3000\n"
     ]
    }
   ],
   "source": [
    "rawArtistData.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artistByID: org.apache.spark.sql.DataFrame = [id: int, name: string]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val artistByID = rawArtistData.flatMap{line => \n",
    "    val (id,name) = line.span(_!='\\t')\n",
    "    if (name.isEmpty){\n",
    "        None} \n",
    "    else {\n",
    "    try{\n",
    "        Some((id.toInt,name.trim))\n",
    "    } catch {\n",
    "        case _: NumberFormatException => None\n",
    "    }\n",
    "    }\n",
    "}toDF(\"id\",\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[org.apache.spark.sql.Row] = Array([1134999,06Crazy Life], [6821360,Pang Nakarin])\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artistByID.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datapath_alias: String = /Users/soda/Downloads/dataSet/aas/profiledata_06-May-2005/artist_alias.txt\n",
       "rawArtistAlias: org.apache.spark.sql.Dataset[String] = [value: string]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datapath_alias = \"/Users/soda/Downloads/dataSet/aas/profiledata_06-May-2005/artist_alias.txt\"\n",
    "val rawArtistAlias = spark.read.textFile(datapath_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1092764\t1000311\n",
      "1095122\t1000557\n",
      "6708070\t1007267\n",
      "10088054\t1042317\n",
      "1195917\t1042317\n"
     ]
    }
   ],
   "source": [
    "rawArtistAlias.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artistAlias: scala.collection.immutable.Map[Int,Int] = Map(1208690 -> 1003926, 2012757 -> 4569, 6949139 -> 1085752, 1109727 -> 1239120, 6772751 -> 1244705, 2070533 -> 1021544, 1157679 -> 2194, 9969617 -> 5630, 2034496 -> 1116214, 6764342 -> 40, 1272489 -> 1278238, 2108744 -> 1009267, 10349857 -> 1000052, 2145319 -> 1020463, 2126338 -> 2717, 10165456 -> 1001169, 6779368 -> 1239506, 10278137 -> 1001523, 9939075 -> 1329390, 2037201 -> 1274155, 1248585 -> 2885, 1106945 -> 1399, 6811322 -> 1019016, 9978396 -> 1784, 6676961 -> 1086433, 2117821 -> 2611, 6863616 -> 1277013, 6895480 -> 1000993, 6831632 -> 1246136, 1001719 -> 1009727, 10135633 -> 4250, 7029291 -> 1034635, 6967939 -> 1002734, 6864694 -> 1017311, 1237279 -> 1029752, 6793956 -> 1283231, 1208609 -> 1000699, 6693428 -> 1100258, 685174..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val artistAlias = rawArtistAlias.flatMap { line =>\n",
    "  val Array(artist, alias) = line.split('\\t')\n",
    "  if (artist.isEmpty) {\n",
    "    None\n",
    "  } else {\n",
    "    Some((artist.toInt, alias.toInt))\n",
    "  }\n",
    "}.collect().toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|     id|            name|\n",
      "+-------+----------------+\n",
      "|1208690|Collective Souls|\n",
      "|1003926| Collective Soul|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artistByID.filter($\"id\" isin (1208690,1003926)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Option[Int] = Some(1003926)\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// https://www.jianshu.com/p/c7b240cabec7\n",
    "artistAlias get 1208690"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4　构建第一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.broadcast._\n",
       "buildCounts: (rawUserArtistData: org.apache.spark.sql.Dataset[String], bArtistAlias: org.apache.spark.broadcast.Broadcast[Map[Int,Int]])org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.broadcast._\n",
    "\n",
    "def buildCounts(\n",
    "    rawUserArtistData: Dataset[String],\n",
    "    bArtistAlias: Broadcast[Map[Int,Int]]): DataFrame = {\n",
    "    rawUserArtistData.map{line =>\n",
    "        val Array(userID,artistID,count) = line.split(\" \").map(_.toInt)\n",
    "        val finalArtistID = \n",
    "                bArtistAlias.value.getOrElse(artistID,artistID)\n",
    "        (userID, finalArtistID ,count)\n",
    "    }.toDF(\"user\",\"artist\",\"count\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bArtistAlias: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[Int,Int]] = Broadcast(17)\n",
       "trainData: org.apache.spark.sql.DataFrame = [user: int, artist: int ... 1 more field]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bArtistAlias = spark.sparkContext.broadcast(artistAlias)\n",
    "val trainData = buildCounts(rawUserArtistData,bArtistAlias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.recommendation._\n",
    "import scala.util.Random\n",
    "val model = new ALS().\n",
    "    setSeed(Random.nextLong()). \n",
    "    setImplicitPrefs(true).\n",
    "    setRank(10).\n",
    "    setRegParam(0.01).\n",
    "    setAlpha(1.0).\n",
    "    setMaxIter(5).\n",
    "    setUserCol(\"user\").\n",
    "    setItemCol(\"artist\").\n",
    "    setRatingCol(\"count\").\n",
    "    setPredictionCol(\"prediction\").\n",
    "    fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
