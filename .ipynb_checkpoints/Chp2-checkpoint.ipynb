{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ hadoop fs -mkdir linkage\n",
    "$ hadoop fs -put block_*.csv linkage\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@22a8369c\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawblocks: org.apache.spark.rdd.RDD[String] = data/linkage/*.csv MapPartitionsRDD[3] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawblocks = sc.textFile(\"data/linkage/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: String = \"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\"\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawblocks.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "head: Array[String] = Array(\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\", 607,53170,1,?,1,?,1,1,1,1,1,TRUE, 88569,88592,1,?,1,?,1,1,1,1,1,TRUE, 21282,26255,1,?,1,?,1,1,1,1,1,TRUE, 20995,42541,1,?,1,?,1,1,1,1,1,TRUE, 27989,34739,1,?,1,?,1,1,1,1,1,TRUE, 32442,69159,1,?,1,?,1,1,1,1,1,TRUE, 24738,29196,1,1,1,?,1,1,1,1,1,TRUE, 9904,89061,1,?,1,?,1,1,1,1,1,TRUE, 29926,36578,1,?,1,?,1,1,1,1,1,TRUE)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val head = rawblocks.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\"\n",
      "607,53170,1,?,1,?,1,1,1,1,1,TRUE\n",
      "88569,88592,1,?,1,?,1,1,1,1,1,TRUE\n",
      "21282,26255,1,?,1,?,1,1,1,1,1,TRUE\n",
      "20995,42541,1,?,1,?,1,1,1,1,1,TRUE\n",
      "27989,34739,1,?,1,?,1,1,1,1,1,TRUE\n",
      "32442,69159,1,?,1,?,1,1,1,1,1,TRUE\n",
      "24738,29196,1,1,1,?,1,1,1,1,1,TRUE\n",
      "9904,89061,1,?,1,?,1,1,1,1,1,TRUE\n",
      "29926,36578,1,?,1,?,1,1,1,1,1,TRUE\n"
     ]
    }
   ],
   "source": [
    "head.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isHeader: (line: String)Boolean\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//过滤标题行\n",
    "def isHeader(line: String) = line.contains(\"id_1\")\n",
    "// 显式地指明函数返回类型\n",
    "// def isHeader(line: String): Boolean = {\n",
    "//     line.contains(\"id_1\")\n",
    "// }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\"\n"
     ]
    }
   ],
   "source": [
    "head.filter(isHeader).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Int = 9\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head.filterNot(isHeader).length\n",
    "//Int=9是因为head.length=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Int = 9\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head.filter(x => !isHeader(x)).length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Int = 9\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head.filter(!isHeader(_)).length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "noheader: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at filter at <console>:28\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val noheader = rawblocks.filter(!isHeader(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: String = 607,53170,1,?,1,?,1,1,1,1,1,TRUE\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noheader.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7　从RDD到DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prev: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prev = spark.read.csv(\"data/linkage/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parsed: org.apache.spark.sql.DataFrame = [id_1: string, id_2: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parsed = spark.read.\n",
    "        option(\"header\",\"true\").\n",
    "        option(\"nullValue\",\"?\").\n",
    "        option(\"inferSchema\",\"true\").\n",
    "        csv(\"data/linkage/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: string (nullable = true)\n",
      " |-- id_2: string (nullable = true)\n",
      " |-- cmp_fname_c1: string (nullable = true)\n",
      " |-- cmp_fname_c2: string (nullable = true)\n",
      " |-- cmp_lname_c1: string (nullable = true)\n",
      " |-- cmp_lname_c2: string (nullable = true)\n",
      " |-- cmp_sex: string (nullable = true)\n",
      " |-- cmp_bd: string (nullable = true)\n",
      " |-- cmp_bm: string (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Long = 5749133\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，当数据可能被多个操作依赖时，并且相对于集群可用的内存和磁盘空间而言，如果数据集较小，而且重新生成的代价很高，那么数据就应该被缓存起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: scala.collection.Map[Boolean,Long] = Map(true -> 20931, false -> 5728202)\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.rdd.\n",
    "    map(_.getAs[Boolean](\"is_match\")).\n",
    "    countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_match|  count|\n",
      "+--------+-------+\n",
      "|   false|5728201|\n",
      "|    true|  20931|\n",
      "|    null|      1|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.\n",
    "    groupBy(\"is_match\").\n",
    "    count().\n",
    "    orderBy($\"count\".desc).\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|      avg(cmp_sex)|stddev_samp(cmp_sex)|\n",
      "+------------------+--------------------+\n",
      "|0.9550012294607436|  0.2073014119031234|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.agg(avg($\"cmp_sex\"),stddev_samp($\"cmp_sex\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.createOrReplaceTempView(\"linkage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_match|    cnt|\n",
      "+--------+-------+\n",
      "|   false|5728201|\n",
      "|    true|  20931|\n",
      "|    null|      1|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select is_match,count(*) cnt\n",
    "    from linkage\n",
    "    group by is_match\n",
    "    order by cnt Desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.9　DataFrame的统计信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val summary = parsed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+\n",
      "|summary|        cmp_fname_c1|       cmp_fname_c2|\n",
      "+-------+--------------------+-------------------+\n",
      "|  count|             5748126|             103699|\n",
      "|   mean|  0.7129023464249419|  0.900008998936421|\n",
      "| stddev| 0.38875843950829186|0.27133067681523776|\n",
      "|    min|                   0|                  0|\n",
      "|    max|2.68694413843136e-05|                  1|\n",
      "+-------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matches: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_1: string, id_2: string ... 10 more fields]\n",
       "matchSummary: org.apache.spark.sql.DataFrame = [summary: string, id_1: string ... 10 more fields]\n",
       "misses: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id_1: string, id_2: string ... 10 more fields]\n",
       "missSummary: org.apache.spark.sql.DataFrame = [summary: string, id_1: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//调查变量与列 is_match 的值之间的相关性\n",
    "val matches = parsed.where(\"is_match=true\")\n",
    "val matchSummary = matches.describe()\n",
    "\n",
    "val misses = parsed.filter($\"is_match\"===false)\n",
    "val missSummary = misses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.10　DataFrame的转置和重塑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- summary: string (nullable = true)\n",
      " |-- id_1: string (nullable = true)\n",
      " |-- id_2: string (nullable = true)\n",
      " |-- cmp_fname_c1: string (nullable = true)\n",
      " |-- cmp_fname_c2: string (nullable = true)\n",
      " |-- cmp_lname_c1: string (nullable = true)\n",
      " |-- cmp_lname_c2: string (nullable = true)\n",
      " |-- cmp_sex: string (nullable = true)\n",
      " |-- cmp_bd: string (nullable = true)\n",
      " |-- cmp_bm: string (nullable = true)\n",
      " |-- cmp_by: string (nullable = true)\n",
      " |-- cmp_plz: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: org.apache.spark.sql.types.StructType = StructType(StructField(summary,StringType,true), StructField(id_1,StringType,true), StructField(id_2,StringType,true), StructField(cmp_fname_c1,StringType,true), StructField(cmp_fname_c2,StringType,true), StructField(cmp_lname_c1,StringType,true), StructField(cmp_lname_c2,StringType,true), StructField(cmp_sex,StringType,true), StructField(cmp_bd,StringType,true), StructField(cmp_bm,StringType,true), StructField(cmp_by,StringType,true), StructField(cmp_plz,StringType,true))\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+\n",
      "|summary|                id_1|               id_2|\n",
      "+-------+--------------------+-------------------+\n",
      "|  count|             5749133|            5749133|\n",
      "|   mean|   33324.47979999771|  66587.42400114964|\n",
      "| stddev|   23659.86139888655|  23620.50188438175|\n",
      "|    min|0.000235404896421846|0.00147710487444609|\n",
      "|    max|                9999|              99999|\n",
      "+-------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.select(\"summary\", \"id_1\", \"id_2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(summary,StringType,true), StructField(id_1,StringType,true), StructField(id_2,StringType,true), StructField(cmp_fname_c1,StringType,true), StructField(cmp_fname_c2,StringType,true), StructField(cmp_lname_c1,StringType,true), StructField(cmp_lname_c2,StringType,true), StructField(cmp_sex,StringType,true), StructField(cmp_bd,StringType,true), StructField(cmp_bm,StringType,true), StructField(cmp_by,StringType,true), StructField(cmp_plz,StringType,true))\n",
       "longForm: org.apache.spark.sql.Dataset[(String, String, Double)] = [_1: string, _2: string ... 1 more field]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = summary.schema\n",
    "val longForm = summary.flatMap(row => {\n",
    "    val metric = row.getString(0) //获取每行的第一个元素即指标名称\n",
    "    (1 until row.size).map(i => {\n",
    "        (metric,schema(i).name,row.getString(i).toDouble)\n",
    "    })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+\n",
      "|   _1|          _2|       _3|\n",
      "+-----+------------+---------+\n",
      "|count|        id_1|5749133.0|\n",
      "|count|        id_2|5749133.0|\n",
      "|count|cmp_fname_c1|5748126.0|\n",
      "|count|cmp_fname_c2| 103699.0|\n",
      "|count|cmp_lname_c1|5749133.0|\n",
      "+-----+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "longForm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+---------+\n",
      "|metric|       field|    value|\n",
      "+------+------------+---------+\n",
      "| count|        id_1|5749133.0|\n",
      "| count|        id_2|5749133.0|\n",
      "| count|cmp_fname_c1|5748126.0|\n",
      "| count|cmp_fname_c2| 103699.0|\n",
      "| count|cmp_lname_c1|5749133.0|\n",
      "+------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "longDF: org.apache.spark.sql.DataFrame = [metric: string, field: string ... 1 more field]\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val longDF = longForm.toDF(\"metric\",\"field\",\"value\")\n",
    "longDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wideDF: org.apache.spark.sql.DataFrame = [field: string, count: double ... 1 more field]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wideDF = longDF.\n",
    "    groupBy(\"field\").\n",
    "    pivot(\"metric\",Seq(\"count\",\"mean\")).\n",
    "    agg(first(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------------+\n",
      "|       field|    count|               mean|\n",
      "+------------+---------+-------------------+\n",
      "|        id_2|5749133.0|  66587.42400114964|\n",
      "|     cmp_plz|5736289.0|0.00552866147434343|\n",
      "|cmp_lname_c1|5749133.0| 0.3156278513776009|\n",
      "|cmp_lname_c2|   2465.0|  0.318296744405166|\n",
      "|     cmp_sex|5749133.0| 0.9550012294607436|\n",
      "+------------+---------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wideDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions.first\n",
       "pivotSummary: (desc: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Pivot.scala\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions.first\n",
    "\n",
    "def pivotSummary(desc: DataFrame): DataFrame = {\n",
    "    val schema = desc.schema\n",
    "    import desc.sparkSession.implicits._\n",
    "    \n",
    "    val lf = desc.flatMap(row => {\n",
    "        val metric = row.getString(0)\n",
    "        (1 until row.size).map(i => {\n",
    "            (metric,schema(i).name,row.getString(i).toDouble)\n",
    "        })\n",
    "    }).toDF(\"metric\",\"field\",\"value\")\n",
    "    lf.groupBy(\"field\").\n",
    "        pivot(\"metric\",Seq(\"count\",\"mean\",\"stddev\",\"min\",\"max\")).\n",
    "        agg(first(\"value\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matchSummaryT: org.apache.spark.sql.DataFrame = [field: string, count: double ... 4 more fields]\n",
       "missSummaryT: org.apache.spark.sql.DataFrame = [field: string, count: double ... 4 more fields]\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val matchSummaryT = pivotSummary(matchSummary)\n",
    "val missSummaryT = pivotSummary(missSummary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.11　DataFrame的连接和特征选择\n",
    "`用 Spark SQL 来表示这些连接会更容易，特别是当待连接的表中有许多列名在两个表中都存在时`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------------+\n",
      "|       field|    total|               delta|\n",
      "+------------+---------+--------------------+\n",
      "|     cmp_plz|5736289.0|  0.9563812499852176|\n",
      "|cmp_lname_c2|   2464.0|  0.8064147192926266|\n",
      "|      cmp_by|5748337.0|  0.7762059675300512|\n",
      "|      cmp_bd|5748337.0|   0.775442311783404|\n",
      "|cmp_lname_c1|5749132.0|  0.6838772482594513|\n",
      "|      cmp_bm|5748337.0|  0.5109496938298685|\n",
      "|cmp_fname_c1|5748125.0|  0.2854529057459947|\n",
      "|cmp_fname_c2| 103698.0| 0.09104268062280174|\n",
      "|     cmp_sex|5749132.0|0.032408185250332844|\n",
      "+------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matchSummaryT.createOrReplaceTempView(\"match_desc\")\n",
    "missSummaryT.createOrReplaceTempView(\"miss_desc\")\n",
    "spark.sql(\"\"\"\n",
    "    select a.field,a.count+b.count as total,a.mean - b.mean as delta\n",
    "    from match_desc a inner join miss_desc b \n",
    "    on a.field = b.field\n",
    "    where a.field not in (\"id_1\",\"id_2\")\n",
    "    order by delta desc, total desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.12　为生产环境准备模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class MatchData\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class MatchData(\n",
    "  id_1: Int,\n",
    "  id_2: Int,\n",
    "  cmp_fname_c1: Option[Double],\n",
    "  cmp_fname_c2: Option[Double],\n",
    "  cmp_lname_c1: Option[Double],\n",
    "  cmp_lname_c2: Option[Double],\n",
    "  cmp_sex: Option[Int],\n",
    "  cmp_bd: Option[Int],\n",
    "  cmp_bm: Option[Int],\n",
    "  cmp_by: Option[Int],\n",
    "  cmp_plz: Option[Int],\n",
    "  is_match: Boolean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.StructField\n",
       "import org.apache.spark.sql.types._\n",
       "cusSchema: org.apache.spark.sql.types.StructType = StructType(StructField(id_1,IntegerType,true), StructField(id_2,IntegerType,true), StructField(cmp_fname_c1,DoubleType,true), StructField(cmp_fname_c2,DoubleType,true), StructField(cmp_lname_c1,DoubleType,true), StructField(cmp_lname_c2,DoubleType,true), StructField(cmp_sex,IntegerType,true), StructField(cmp_bd,IntegerType,true), StructField(cmp_bm,IntegerType,true), StructField(cmp_by,IntegerType,true), StructField(cmp_plz,IntegerType,true), StructField(is_match,BooleanType,true))\n",
       "df: org.apache.spark.sql.DataFrame = [id_1: int, id_2: int ... 10 more fields]\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.StructField\n",
    "import org.apache.spark.sql.types._\n",
    "val cusSchema = StructType(Array(\n",
    "    StructField(\"id_1\",IntegerType,true),\n",
    "    StructField(\"id_2\",IntegerType,true),\n",
    "    StructField(\"cmp_fname_c1\",DoubleType,true),\n",
    "    StructField(\"cmp_fname_c2\",DoubleType,true),\n",
    "    StructField(\"cmp_lname_c1\",DoubleType,true),\n",
    "    StructField(\"cmp_lname_c2\",DoubleType,true),\n",
    "    StructField(\"cmp_sex\",IntegerType,true),\n",
    "    StructField(\"cmp_bd\",IntegerType,true),\n",
    "    StructField(\"cmp_bm\",IntegerType,true),\n",
    "    StructField(\"cmp_by\",IntegerType,true),\n",
    "    StructField(\"cmp_plz\",IntegerType,true),\n",
    "    StructField(\"is_match\",BooleanType,true)\n",
    "))\n",
    "\n",
    "val df = spark.read.\n",
    "    option(\"header\",\"true\").\n",
    "    option(\"nullValue\",\"?\").\n",
    "    schema(cusSchema).\n",
    "    csv(\"data/linkage/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matchData: org.apache.spark.sql.Dataset[MatchData] = [id_1: int, id_2: int ... 10 more fields]\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val matchData = df.as[MatchData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|14055|94934|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matchData.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matchData 这个 Dataset 中所有的列和值与 parsed 这个 DataFrame 中的数据是一样的，我们仍然可以对 matchData 使用所有 SQL 风格的 DataFrame API 方法以及 Spark SQL 代码。两者之间的主要区别是，当我们对 matchData 调用函数时，例如 map、flatMap 和 filter，我们处理的是 MatchData 这个 case 类，而不是 Row 类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Score\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Score(value: Double){\n",
    "    def +(oi: Option[Int])={\n",
    "        Score(value + oi.getOrElse(0))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scoreMatchData: (md: MatchData)Double\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scoreMatchData(md: MatchData): Double = {\n",
    "    (Score(md.cmp_lname_c1.getOrElse(0.0)) +\n",
    "     md.cmp_plz + md.cmp_by + md.cmp_bd + md.cmp_bm).value\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "64: error: type mismatch;",
     "output_type": "error",
     "traceback": [
      "<console>:64: error: type mismatch;",
      " found   : org.apache.spark.sql.Dataset[MatchData]",
      " required: MatchData",
      "       scoreMatchData(matchData)",
      "                      ^",
      ""
     ]
    }
   ],
   "source": [
    "⚠️scoreMatchData(matchData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scored: org.apache.spark.sql.DataFrame = [score: double, is_match: boolean]\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scored = matchData.map{md => \n",
    "(scoreMatchData(md),md.is_match)\n",
    "}.toDF(\"score\",\"is_match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.13　评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossTabs: (scored: org.apache.spark.sql.DataFrame, t: Double)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crossTabs(scored: DataFrame, t: Double): DataFrame = {\n",
    "  scored.\n",
    "    selectExpr(s\"score >= $t as above\", \"is_match\").\n",
    "    groupBy(\"above\").\n",
    "    pivot(\"is_match\", Seq(\"true\", \"false\")).\n",
    "    count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 1 in stage 57.0 failed 1 times, most recent failure: Lost task 1.0 in stage 57.0 (TID 1686, localhost, executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 57.0 failed 1 times, most recent failure: Lost task 1.0 in stage 57.0 (TID 1686, localhost, executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:719)",
      "  ... 38 elided",
      "Caused by: java.lang.ClassCastException: $iw cannot be cast to $iw",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)",
      "  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "crossTabs(scored, 4.0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"matchdataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.sql.DataFrame = [score: double, is_match: boolean]\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = spark.sql(\"\"\"\n",
    "    select cmp_lname_c1 + cmp_plz + cmp_by + cmp_bd + cmp_bm as score,\n",
    "    is_match\n",
    "    from matchdataframe\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+\n",
      "|above| true|  false|\n",
      "+-----+-----+-------+\n",
      "| null|   35|  13603|\n",
      "| true|20842|    636|\n",
      "|false|   54|5713962|\n",
      "+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crossTabs(result, 4.0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [CASE CLASSES](https://docs.scala-lang.org/zh-cn/tour/case-classes.html)\n",
    "- [Create a Dataset from an RDD](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.16.8.92:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1582682941971)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defined class Book\n",
       "frankenstein: Book = Book(978-0486282114)\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Book(isbn: String)\n",
    "val frankenstein = Book(\"978-0486282114\")\n",
    "frankenstein.isbn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//Create a Dataset from a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+\n",
      "|name|foundingYear|numEmployees|\n",
      "+----+------------+------------+\n",
      "| ABC|        1980|         310|\n",
      "| XYZ|        1983|         904|\n",
      "| NOP|        2005|          83|\n",
      "+----+------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "import spark.implicits._\n",
       "defined class Company\n",
       "inputSeq: Seq[Company] = List(Company(ABC,1980,310), Company(XYZ,1983,904), Company(NOP,2005,83))\n",
       "df: org.apache.spark.sql.DataFrame = [name: string, foundingYear: int ... 1 more field]\n",
       "companyDS: org.apache.spark.sql.Dataset[Company] = [name: string, foundingYear: int ... 1 more field]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import spark.implicits._\n",
    "\n",
    "case class Company(name: String, foundingYear: Int, numEmployees: Int)\n",
    "val inputSeq = Seq(Company(\"ABC\",1980,310),\n",
    "                   Company(\"XYZ\",1983,904),\n",
    "                   Company(\"NOP\",2005,83))\n",
    "\n",
    "val df = inputSeq.toDS().toDF()\n",
    "// val df = sc.parallelize(inputSeq).toDF() //←报错\n",
    "val companyDS = df.as[Company]\n",
    "companyDS.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
