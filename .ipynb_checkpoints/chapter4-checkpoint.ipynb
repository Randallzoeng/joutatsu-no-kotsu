{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.SparkContext = org.apache.spark.SparkContext@60742327\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = /Users/soda/Downloads/backUp/Notebooks/AdvancedAnalyticswithSpark/data/\n",
       "dataWithoutHeader: org.apache.spark.sql.DataFrame = [_c0: int, _c1: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"/Users/soda/Downloads/backUp/Notebooks/AdvancedAnalyticswithSpark/data/\"\n",
    "// val path = \"hdfs://soda-lab-bigdata-01:8020/dataset/chapter3/\"\n",
    "val dataWithoutHeader = spark.read.\n",
    "    option(\"inferSchema\",true).\n",
    "    option(\"header\",false).\n",
    "    csv(path+\"covtype.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colNames: Seq[String] = List(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, Soil_..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colNames = Seq(\n",
    "    \"Elevation\", \"Aspect\", \"Slope\",\n",
    "    \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\"\n",
    "  ) ++ ( \n",
    "    (0 until 4).map(i => s\"Wilderness_Area_$i\")\n",
    "  ) ++ (\n",
    "    (0 until 40).map(i => s\"Soil_Type_$i\")\n",
    "  ) ++ Seq(\"Cover_Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 在 Spark MLlib 所有 API 中，目标列通常都被视为双精度浮点数类型而不是整型\n",
    "val data = dataWithoutHeader.toDF(colNames:_*).\n",
    "    withColumn(\"Cover_Type\",$\"Cover_Type\".cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.Row = [2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5.0]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.7　第一棵决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\n",
       "res3: testData.type = [Elevation: int, Aspect: int ... 53 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainData,testData) = data.randomSplit(Array(0.9,0.1))\n",
    "trainData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark MLlib 要求将所有输入合并成一列，该列的值是一个向量。这个类是线性代数中向量的一个抽象，仅包含一些数字。对于大多数的意图和目的，向量就像是一个简单的双精度浮点数数组。当然，有些输入特征在概念上是类别型的，即使在输入中它们都是用数值表示的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "inputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_T..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val inputCols = trainData.columns.filter( _!=\"Cover_Type\")\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(inputCols).\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "val assemblerTrainData = assembler.transform(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|featureVector                                                                                        |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1863.0,37.0,17.0,120.0,18.0,90.0,217.0,202.0,115.0,769.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,5,6,7,8,9,13,18],[1874.0,18.0,14.0,90.0,208.0,209.0,135.0,793.0,1.0,1.0])                 |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1879.0,28.0,19.0,30.0,12.0,95.0,209.0,196.0,117.0,778.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1889.0,28.0,22.0,150.0,23.0,120.0,205.0,185.0,108.0,759.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1889.0,353.0,30.0,95.0,39.0,67.0,153.0,172.0,146.0,600.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1896.0,337.0,12.0,30.0,6.0,175.0,195.0,224.0,168.0,732.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1898.0,34.0,23.0,175.0,56.0,134.0,210.0,184.0,99.0,765.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1899.0,355.0,22.0,153.0,43.0,124.0,178.0,195.0,151.0,819.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1901.0,311.0,9.0,30.0,2.0,190.0,195.0,234.0,179.0,726.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1903.0,5.0,13.0,42.0,4.0,201.0,203.0,214.0,148.0,708.0,1.0,1.0])    |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1904.0,51.0,26.0,67.0,30.0,162.0,222.0,175.0,72.0,711.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1905.0,33.0,27.0,90.0,46.0,150.0,204.0,171.0,89.0,725.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1905.0,77.0,21.0,90.0,38.0,120.0,241.0,196.0,75.0,1025.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1906.0,356.0,20.0,150.0,55.0,120.0,184.0,201.0,151.0,726.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1908.0,323.0,32.0,150.0,52.0,120.0,125.0,190.0,196.0,765.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1916.0,24.0,25.0,212.0,74.0,175.0,197.0,177.0,105.0,789.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1916.0,320.0,24.0,190.0,60.0,162.0,151.0,210.0,195.0,832.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1919.0,30.0,22.0,67.0,9.0,256.0,208.0,188.0,107.0,661.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1919.0,44.0,26.0,162.0,68.0,150.0,216.0,173.0,77.0,706.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1922.0,50.0,29.0,108.0,43.0,218.0,219.0,165.0,62.0,659.0,1.0,1.0])  |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assemblerTrainData.select(\"featureVector\").show(truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
       "import scala.util.Random\n",
       "classifier: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_6063bb601139\n",
       "model: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel (uid=dtc_6063bb601139) of depth 5 with 47 nodes\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import scala.util.Random\n",
    "\n",
    "val classifier = new DecisionTreeClassifier().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setFeaturesCol(\"featureVector\").\n",
    "    setPredictionCol(\"prediction\")\n",
    "\n",
    "val model = classifier.fit(assemblerTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=dtc_6063bb601139) of depth 5 with 47 nodes\n",
      "  If (feature 0 <= 3054.5)\n",
      "   If (feature 0 <= 2551.5)\n",
      "    If (feature 10 <= 0.5)\n",
      "     If (feature 0 <= 2440.5)\n",
      "      If (feature 3 <= 15.0)\n",
      "       Predict: 4.0\n",
      "      Else (feature 3 > 15.0)\n",
      "       Predict: 3.0\n",
      "     Else (feature 0 > 2440.5)\n",
      "      Predict: 3.0\n",
      "    Else (feature 10 > 0.5)\n",
      "     If (feature 9 <= 5418.5)\n",
      "      Predict: 2.0\n",
      "     Else (feature 9 > 5418.5)\n",
      "      If (feature 5 <= 562.0)\n",
      "       Predict: 2.0\n",
      "      Else (feature 5 > 562.0)\n",
      "       Predict: 5.0\n",
      "   Else (feature 0 > 2551.5)\n",
      "    If (feature 0 <= 2958.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      If (feature 17 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 17 > 0.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      Predict: 3.0\n",
      "    Else (feature 0 > 2958.5)\n",
      "     If (feature 3 <= 211.0)\n",
      "      If (feature 36 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 36 > 0.5)\n",
      "       Predict: 1.0\n",
      "     Else (feature 3 > 211.0)\n",
      "      Predict: 2.0\n",
      "  Else (feature 0 > 3054.5)\n",
      "   If (feature 0 <= 3310.5)\n",
      "    If (feature 7 <= 239.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      Predict: 1.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 3340.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 5 > 3340.5)\n",
      "       Predict: 1.0\n",
      "    Else (feature 7 > 239.5)\n",
      "     If (feature 3 <= 316.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 316.0)\n",
      "      If (feature 0 <= 3204.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 3204.5)\n",
      "       Predict: 1.0\n",
      "   Else (feature 0 > 3310.5)\n",
      "    If (feature 12 <= 0.5)\n",
      "     If (feature 3 <= 298.5)\n",
      "      If (feature 6 <= 208.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 6 > 208.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 3 > 298.5)\n",
      "      Predict: 1.0\n",
      "    Else (feature 12 > 0.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      Predict: 7.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 992.5)\n",
      "       Predict: 7.0\n",
      "      Else (feature 5 > 992.5)\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8009876265452001,Elevation)\n",
      "(0.04193970897877591,Horizontal_Distance_To_Hydrology)\n",
      "(0.028150140885279895,Soil_Type_31)\n",
      "(0.026473232904648005,Soil_Type_1)\n",
      "(0.026016157281142902,Wilderness_Area_0)\n",
      "(0.02506509684416583,Hillshade_Noon)\n",
      "(0.023803965265474125,Soil_Type_3)\n",
      "(0.011862336917403741,Wilderness_Area_2)\n",
      "(0.006275538165483466,Soil_Type_22)\n",
      "(0.0062207782575881745,Horizontal_Distance_To_Roadways)\n",
      "(0.0025350620224685715,Hillshade_9am)\n",
      "(6.703559323692008E-4,Horizontal_Distance_To_Fire_Points)\n",
      "(0.0,Wilderness_Area_3)\n",
      "(0.0,Wilderness_Area_1)\n",
      "(0.0,Vertical_Distance_To_Hydrology)\n",
      "(0.0,Soil_Type_9)\n",
      "(0.0,Soil_Type_8)\n",
      "(0.0,Soil_Type_7)\n",
      "(0.0,Soil_Type_6)\n",
      "(0.0,Soil_Type_5)\n",
      "(0.0,Soil_Type_4)\n",
      "(0.0,Soil_Type_39)\n",
      "(0.0,Soil_Type_38)\n",
      "(0.0,Soil_Type_37)\n",
      "(0.0,Soil_Type_36)\n",
      "(0.0,Soil_Type_35)\n",
      "(0.0,Soil_Type_34)\n",
      "(0.0,Soil_Type_33)\n",
      "(0.0,Soil_Type_32)\n",
      "(0.0,Soil_Type_30)\n",
      "(0.0,Soil_Type_29)\n",
      "(0.0,Soil_Type_28)\n",
      "(0.0,Soil_Type_27)\n",
      "(0.0,Soil_Type_26)\n",
      "(0.0,Soil_Type_25)\n",
      "(0.0,Soil_Type_24)\n",
      "(0.0,Soil_Type_23)\n",
      "(0.0,Soil_Type_21)\n",
      "(0.0,Soil_Type_20)\n",
      "(0.0,Soil_Type_2)\n",
      "(0.0,Soil_Type_19)\n",
      "(0.0,Soil_Type_18)\n",
      "(0.0,Soil_Type_17)\n",
      "(0.0,Soil_Type_16)\n",
      "(0.0,Soil_Type_15)\n",
      "(0.0,Soil_Type_14)\n",
      "(0.0,Soil_Type_13)\n",
      "(0.0,Soil_Type_12)\n",
      "(0.0,Soil_Type_11)\n",
      "(0.0,Soil_Type_10)\n",
      "(0.0,Soil_Type_0)\n",
      "(0.0,Slope)\n",
      "(0.0,Hillshade_3pm)\n",
      "(0.0,Aspect)\n"
     ]
    }
   ],
   "source": [
    "model.featureImportances.toArray.zip(inputCols).\n",
    "    sorted.reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 57 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(assemblerTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------------------------------------------------------------------------------------+\n",
      "|Cover_Type|prediction|probability                                                                                      |\n",
      "+----------+----------+-------------------------------------------------------------------------------------------------+\n",
      "|6.0       |3.0       |[0.0,0.0,0.030218102116345083,0.6357658725881211,0.05199238259494808,0.0,0.28202364270058566,0.0]|\n",
      "|6.0       |4.0       |[0.0,0.0,0.04048582995951417,0.29104813315339634,0.440395861448493,0.0,0.22807017543859648,0.0]  |\n",
      "|6.0       |3.0       |[0.0,0.0,0.030218102116345083,0.6357658725881211,0.05199238259494808,0.0,0.28202364270058566,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.030218102116345083,0.6357658725881211,0.05199238259494808,0.0,0.28202364270058566,0.0]|\n",
      "|6.0       |3.0       |[0.0,0.0,0.030218102116345083,0.6357658725881211,0.05199238259494808,0.0,0.28202364270058566,0.0]|\n",
      "+----------+----------+-------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"Cover_Type\",\"prediction\",\"probability\").show(5,truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_1905581764ee\n",
       "res8: Double = 0.683033942836437\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator().\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setPredictionCol(\"prediction\")\n",
    "\n",
    "evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "evaluator.setMetricName(\"f1\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "predictionRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[98] at rdd at <console>:35\n",
       "multiclassMetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@6b13545c\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "\n",
    "val predictionRDD = predictions.\n",
    "    select(\"prediction\",\"Cover_Type\").\n",
    "    as[(Double,Double)].\n",
    "    rdd\n",
    "\n",
    "val multiclassMetrics = new MulticlassMetrics(predictionRDD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.mllib.linalg.Matrix =\n",
       "124756.0  60009.0   219.0    0.0    0.0   0.0  5594.0\n",
       "45370.0   200763.0  7736.0   90.0   30.0  0.0  839.0\n",
       "0.0       2379.0    29160.0  647.0  0.0   0.0  0.0\n",
       "0.0       0.0       1474.0   979.0  0.0   0.0  0.0\n",
       "2.0       7690.0    775.0    0.0    88.0  0.0  0.0\n",
       "0.0       3209.0    11987.0  507.0  0.0   0.0  0.0\n",
       "7819.0    79.0      54.0     0.0    0.0   0.0  10449.0\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclassMetrics.confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confusionMatrix: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Cover_Type: double, 1: bigint ... 6 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val confusionMatrix = predictions.groupBy(\"Cover_Type\").\n",
    "    pivot(\"prediction\",(1 to 7)).\n",
    "    count().\n",
    "    na.fill(0.0).\n",
    "    orderBy(\"Cover_Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-----+---+---+---+-----+\n",
      "|Cover_Type|     1|     2|    3|  4|  5|  6|    7|\n",
      "+----------+------+------+-----+---+---+---+-----+\n",
      "|       1.0|124756| 60009|  219|  0|  0|  0| 5594|\n",
      "|       2.0| 45370|200763| 7736| 90| 30|  0|  839|\n",
      "|       3.0|     0|  2379|29160|647|  0|  0|    0|\n",
      "|       4.0|     0|     0| 1474|979|  0|  0|    0|\n",
      "|       5.0|     2|  7690|  775|  0| 88|  0|    0|\n",
      "|       6.0|     0|  3209|11987|507|  0|  0|    0|\n",
      "|       7.0|  7819|    79|   54|  0|  0|  0|10449|\n",
      "+----------+------+------+-----+---+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusionMatrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "classProbabilities: (data: org.apache.spark.sql.DataFrame)Array[Double]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def classProbabilities(data: DataFrame): Array[Double] = {\n",
    "    val total = data.count()\n",
    "    data.groupBy(\"Cover_Type\").count().\n",
    "        orderBy(\"Cover_Type\").\n",
    "        select(\"count\").as[Double].\n",
    "        map(_ / total).\n",
    "        collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainPriorProb: Array[Double] = Array(0.3646002326364443, 0.48751874866080996, 0.06157595886008142, 0.004692904588447764, 0.016366815635617864, 0.030041859254951175, 0.035203480363647496)\n",
       "testPriorProb: Array[Double] = Array(0.3646497907662756, 0.48832064210742954, 0.061192289222748164, 0.005042189750977567, 0.016086986348357, 0.028538107978322014, 0.036169993825890104)\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainPriorProb = classProbabilities(trainData)\n",
    "val testPriorProb = classProbabilities(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Double = 0.377202443559211\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainPriorProb.zip(testPriorProb).map { \n",
    "  case (trainProb, cvProb) => trainProb * cvProb\n",
    "}.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": " error: incomplete input",
     "output_type": "error",
     "traceback": [
      "<console>: error: incomplete input"
     ]
    }
   ],
   "source": [
    "do not run!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.8　决策树的超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val inputCols = trainData.columns.filter(_ != \"Cover_Type\")\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(inputCols).\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "val classifier = new DecisionTreeClassifier().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setFeaturesCol(\"featureVector\").\n",
    "    setPredictionCol(\"prediction\")\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler,classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "    addGrid(classifier.impurity,Seq(\"gini\",\"entropy\")).\n",
    "    addGrid(classifier.maxDepth,Seq(1,20)).\n",
    "    addGrid(classifier.minInfoGain,Seq(0.0,0.05)).build()\n",
    "\n",
    "val multiclassEval = new MulticlassClassificationEvaluator().\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setPredictionCol(\"prediction\").\n",
    "    setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
    "\n",
    "val validator = new TrainValidationSplit().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setEstimator(pipeline).\n",
    "    setEvaluator(multiclassEval).\n",
    "    setEstimatorParamMaps(paramGrid).\n",
    "    setTrainRatio(0.9)\n",
    "\n",
    "val validationModel = validator.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.PipelineModel\n",
    "val bestModel = validationModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val validatorModel = validator.fit(trainData)\n",
    "\n",
    "val paramsAndMetrics = validatorModel.validationMetrics.\n",
    "  zip(validatorModel.getEstimatorParamMaps).sortBy(-_._1)\n",
    "\n",
    "paramsAndMetrics.foreach { case (metric, params) =>\n",
    "    println(metric)\n",
    "    println(params)\n",
    "    println()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validatorModel.validationMetrics.max\n",
    "multiclassEval.evaluate(bestModel.transform(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.10　重谈类别型特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "unencodeOneHot: (data: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "\n",
    "def unencodeOneHot(data: DataFrame): DataFrame = {\n",
    "    val wildernessCols = (0 until 4).map(i => s\"Wilderness_Area_$i\").toArray\n",
    "    val wildernessAssembler = new VectorAssembler().\n",
    "        setInputCols(wildernessCols).\n",
    "        setOutputCol(\"wilderness\")\n",
    "    \n",
    "    val unhotUDF = udf((vec: Vector)=> vec.toArray.indexOf(1.0).toDouble)\n",
    "    val withWilderness = wildernessAssembler.transform(data).\n",
    "        drop(wildernessCols:_*).\n",
    "        withColumn(\"wilderness\",unhotUDF($\"wilderness\"))\n",
    "    \n",
    "    val soilCols = (0 until 40).map(i=>s\"Soil_Type_$i\").toArray\n",
    "    val soilAssembler = new VectorAssembler().\n",
    "        setInputCols(soilCols).\n",
    "        setOutputCol(\"soil\")\n",
    "    \n",
    "    soilAssembler.transform(withWilderness).\n",
    "        drop(soilCols:_*).\n",
    "        withColumn(\"soil\",unhotUDF($\"soil\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unencTrainData: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 11 more fields]\n",
       "unencTestData: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unencTrainData = unencodeOneHot(trainData)\n",
    "val unencTestData = unencodeOneHot(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+----------+----------+----+\n",
      "|Elevation|Aspect|Slope|Horizontal_Distance_To_Hydrology|Vertical_Distance_To_Hydrology|Horizontal_Distance_To_Roadways|Hillshade_9am|Hillshade_Noon|Hillshade_3pm|Horizontal_Distance_To_Fire_Points|Cover_Type|wilderness|soil|\n",
      "+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+----------+----------+----+\n",
      "|     1863|    37|   17|                             120|                            18|                             90|          217|           202|          115|                               769|       6.0|       3.0| 1.0|\n",
      "|     1874|    18|   14|                               0|                             0|                             90|          208|           209|          135|                               793|       6.0|       3.0| 4.0|\n",
      "+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+----------+----------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unencTrainData.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个新的数值列没有任何信息表明它们实际上是类别型特征值的编码。把它们当作数值型是错误的，因为它们的大小是没有意义的。然而，这种处理方式似乎还能奏效，好像什么事情都没发生一样，不过这些特征的信息可能在处理过程中就完全丢失了。\n",
    "\n",
    "Spark MLlib 内部可以存储每列额外的元数据信息。这些数据的细节通常对调用者来说是隐藏的，诸如列是否对类别型特征值编码，以及类别型编码有多少不同的值。为了添加这些元数据，有必要通过 VectorIndexer 存入这些数据。VectorIndexer 的目标是将输入变成有正确标签的类别型特征列。尽管我们已经做了很多工作来将类别型特征转换为从 0 开始的索引值，VectorIndexer 将会负责生成元数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorIndexer\n",
       "import org.apache.spark.ml.Pipeline\n",
       "inputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, wilderness, soil)\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_52ee9d5d74f5\n",
       "indexer: org.apache.spark.ml.feature.VectorIndexer = vecIdx_d97f95e7ebf7\n",
       "classifier: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_916d7d65ea9b\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_f804fdb9c1aa\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorIndexer\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val inputCols = unencTrainData.columns.filter(_!=\"Cover_Type\")\n",
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(inputCols).\n",
    "    setOutputCol(\"featureVector\")\n",
    "\n",
    "val indexer = new VectorIndexer().\n",
    "    setMaxCategories(40).\n",
    "    setInputCol(\"featureVector\").\n",
    "    setOutputCol(\"indexedVector\")\n",
    "\n",
    "val classifier = new DecisionTreeClassifier().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setFeaturesCol(\"indexedVector\").\n",
    "    setPredictionCol(\"prediction\")\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(assembler,indexer,classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.11　随机决策森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.RandomForestClassifier\n",
       "classifier: org.apache.spark.ml.classification.RandomForestClassifier = rfc_64fdc980a317\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "\n",
    "val classifier = new RandomForestClassifier().\n",
    "    setSeed(Random.nextLong()).\n",
    "    setLabelCol(\"Cover_Type\").\n",
    "    setFeaturesCol(\"indexedVector\").\n",
    "    setPredictionCol(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// import org.apache.spark.ml.classification.RandomForestClassificationModel\n",
    "\n",
    "// val forestModel = bestModel.asInstanceOf[PipelineModel].\n",
    "//   stages.last.asInstanceOf[RandomForestClassificationModel]\n",
    "\n",
    "// forestModel.featureImportances.toArray.zip(inputCols).\n",
    "//   sorted.reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel.transform(unencTestData.drop(\"Cover_Type\")).select(\"prediction\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
