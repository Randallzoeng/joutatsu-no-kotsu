{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// http://allaboutscala.com/big-data/spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataFrame from reading a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.16.8.92:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1587535826775)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
       "import org.apache.spark.{SparkConf, SparkContext}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, Row, SparkSession}\n",
    "import org.apache.spark.{SparkConf, SparkContext}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_data: String = /Users/soda/Documents/GitHub/joutatsu-no-kotsu/resources/question_tags_10K.csv\n",
       "dfTags: org.apache.spark.sql.DataFrame = [id: int, tag: string]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path_data = \"/Users/soda/Documents/GitHub/joutatsu-no-kotsu/resources/question_tags_10K.csv\"\n",
    "val dfTags = spark\n",
    "    .read\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .csv(path_data)\n",
    "    .toDF(\"id\",\"tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|                tag|\n",
      "+---+-------------------+\n",
      "|  1|               data|\n",
      "|  4|                 c#|\n",
      "|  4|           winforms|\n",
      "|  4|    type-conversion|\n",
      "|  4|            decimal|\n",
      "|  4|            opacity|\n",
      "|  6|               html|\n",
      "|  6|                css|\n",
      "|  6|               css3|\n",
      "|  6|internet-explorer-7|\n",
      "+---+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTags.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTags.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                tag|\n",
      "+-------------------+\n",
      "|               data|\n",
      "|                 c#|\n",
      "|           winforms|\n",
      "|    type-conversion|\n",
      "|            decimal|\n",
      "|            opacity|\n",
      "|               html|\n",
      "|                css|\n",
      "|               css3|\n",
      "|internet-explorer-7|\n",
      "+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: select columns from a dataframe\n",
    "dfTags.select(\"tag\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|tag|\n",
      "+---+---+\n",
      "| 23|php|\n",
      "| 42|php|\n",
      "| 85|php|\n",
      "|126|php|\n",
      "|146|php|\n",
      "+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: filter by column value of a dataframe\n",
    "dfTags.filter(\"tag=='php'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of php tags = 133\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: count rows of a dataframe\n",
    "println(s\"Number of php tags = ${dfTags.filter(\"tag == 'php'\").count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id|          tag|\n",
      "+---+-------------+\n",
      "| 25|      sockets|\n",
      "| 36|          sql|\n",
      "| 36|   sql-server|\n",
      "| 40| structuremap|\n",
      "| 48|submit-button|\n",
      "+---+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: SQL like query\n",
    "dfTags.filter(\"tag like 's%'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|    tag|\n",
      "+---+-------+\n",
      "| 25|sockets|\n",
      "|108|    svn|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: Multiple filter chaining\n",
    "dfTags\n",
    "    .filter(\"tag like 's%'\")\n",
    "    .filter(\"id == 25 or id == 108\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|      tag|\n",
      "+---+---------+\n",
      "| 25|      c++|\n",
      "| 25|        c|\n",
      "| 25|  sockets|\n",
      "| 25|mainframe|\n",
      "| 25|      zos|\n",
      "+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: SQL IN clause\n",
    "dfTags.filter(\"id in (25,108)\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group by tag value\n",
      "+--------------------+-----+\n",
      "|                 tag|count|\n",
      "+--------------------+-----+\n",
      "|         type-safety|    4|\n",
      "|             jbutton|    1|\n",
      "|              iframe|    2|\n",
      "|           svn-hooks|    2|\n",
      "|           standards|    7|\n",
      "|knowledge-management|    2|\n",
      "|            trayicon|    1|\n",
      "|           arguments|    1|\n",
      "|                 zfs|    1|\n",
      "|              import|    3|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: SQL Group By\n",
    "println(\"Group by tag value\")\n",
    "dfTags.groupBy(\"tag\").count().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|          tag|count|\n",
      "+-------------+-----+\n",
      "|    standards|    7|\n",
      "|     keyboard|    8|\n",
      "|          rss|   12|\n",
      "|documentation|   15|\n",
      "|      session|    6|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: SQL Group By with filter\n",
    "dfTags.groupBy(\"tag\").count().filter(\"count > 5\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|     tag|count|\n",
      "+--------+-----+\n",
      "|    .net|  351|\n",
      "|.net-2.0|   14|\n",
      "+--------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: SQL order by\n",
    "dfTags.groupBy(\"tag\").count().filter(\"count >5\")\n",
    "    .orderBy(\"tag\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfQuestionCSV: org.apache.spark.sql.DataFrame = [id: int, creation_date: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DataFrame Query: Cast columns to specific data type\n",
    "val dfQuestionCSV = spark\n",
    "    .read\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .option(\"dateFormat\",\"yyyy-MM-dd HH:mm:ss\")\n",
    "    .csv(\"/Users/soda/Documents/GitHub/joutatsu-no-kotsu/resources/questions_10K.csv\")\n",
    "    .toDF(\"id\",\"creation_date\",\"closed_date\", \"deletion_date\", \n",
    "          \"score\", \"owner_userid\", \"answer_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- creation_date: timestamp (nullable = true)\n",
      " |-- closed_date: string (nullable = true)\n",
      " |-- deletion_date: string (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- owner_userid: string (nullable = true)\n",
      " |-- answer_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestionCSV.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+--------------------+-----+------------+------------+\n",
      "| id|      creation_date|closed_date|       deletion_date|score|owner_userid|answer_count|\n",
      "+---+-------------------+-----------+--------------------+-----+------------+------------+\n",
      "|  1|2008-08-01 05:26:37|         NA|2011-03-28T00:53:47Z|    1|          NA|           0|\n",
      "|  4|2008-08-01 05:42:52|         NA|                  NA|  472|           8|          13|\n",
      "+---+-------------------+-----------+--------------------+-----+------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestionCSV.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfQuestions: org.apache.spark.sql.DataFrame = [id: int, creation_date: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfQuestions = dfQuestionCSV.select(\n",
    "    dfQuestionCSV.col(\"id\").cast(\"integer\"),\n",
    "    dfQuestionCSV.col(\"creation_date\").cast(\"timestamp\"),\n",
    "    dfQuestionCSV.col(\"closed_date\").cast(\"timestamp\"),\n",
    "    dfQuestionCSV.col(\"deletion_date\").cast(\"date\"),\n",
    "    dfQuestionCSV.col(\"score\").cast(\"integer\"),\n",
    "    dfQuestionCSV.col(\"owner_userid\").cast(\"integer\"),\n",
    "    dfQuestionCSV.col(\"answer_count\").cast(\"integer\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- creation_date: timestamp (nullable = true)\n",
      " |-- closed_date: timestamp (nullable = true)\n",
      " |-- deletion_date: date (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- owner_userid: integer (nullable = true)\n",
      " |-- answer_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestions.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+-------------+-----+------------+------------+\n",
      "| id|      creation_date|closed_date|deletion_date|score|owner_userid|answer_count|\n",
      "+---+-------------------+-----------+-------------+-----+------------+------------+\n",
      "|  1|2008-08-01 05:26:37|       null|   2011-03-28|    1|        null|           0|\n",
      "|  4|2008-08-01 05:42:52|       null|         null|  472|           8|          13|\n",
      "|  6|2008-08-01 06:08:08|       null|         null|  210|           9|           5|\n",
      "+---+-------------------+-----------+-------------+-----+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "|  id|      creation_date|        closed_date|deletion_date|score|owner_userid|answer_count|\n",
      "+----+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "| 888|2008-08-04 07:18:21|2016-08-04 17:22:00|         null|  405|         131|          30|\n",
      "|1939|2008-08-05 13:39:36|2012-06-05 21:13:38|   2012-12-18|  408|        null|          48|\n",
      "+----+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfQuestionsSubset: org.apache.spark.sql.DataFrame = [id: int, creation_date: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DataFrame Query: Operate on a sliced dataframe\n",
    "val dfQuestionsSubset = dfQuestions.filter(\"score > 400 and score < 410\").toDF()\n",
    "dfQuestionsSubset.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------------+-----+\n",
      "|owner_userid|      tag|      creation_date|score|\n",
      "+------------+---------+-------------------+-----+\n",
      "|         131|      php|2008-08-04 07:18:21|  405|\n",
      "|         131|  eclipse|2008-08-04 07:18:21|  405|\n",
      "|         131|debugging|2008-08-04 07:18:21|  405|\n",
      "|         131| phpstorm|2008-08-04 07:18:21|  405|\n",
      "|         131|   xdebug|2008-08-04 07:18:21|  405|\n",
      "+------------+---------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: Join\n",
    "dfQuestionsSubset.join(dfTags,\"id\")\n",
    "    .select(\"owner_userid\",\"tag\",\"creation_date\",\"score\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------------+-----+\n",
      "|owner_userid|      tag|      creation_date|score|\n",
      "+------------+---------+-------------------+-----+\n",
      "|         131|      php|2008-08-04 07:18:21|  405|\n",
      "|         131|  eclipse|2008-08-04 07:18:21|  405|\n",
      "|         131|debugging|2008-08-04 07:18:21|  405|\n",
      "+------------+---------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: Join on explicit columns\n",
    "dfQuestionsSubset\n",
    "    .join(dfTags,dfTags(\"id\")===dfQuestionsSubset(\"id\"))\n",
    "    .select(\"owner_userid\",\"tag\",\"creation_date\",\"score\")\n",
    "    .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------------+-----+\n",
      "|owner_userid|      tag|      creation_date|score|\n",
      "+------------+---------+-------------------+-----+\n",
      "|         131|      php|2008-08-04 07:18:21|  405|\n",
      "|         131|  eclipse|2008-08-04 07:18:21|  405|\n",
      "|         131|debugging|2008-08-04 07:18:21|  405|\n",
      "|         131| phpstorm|2008-08-04 07:18:21|  405|\n",
      "|         131|   xdebug|2008-08-04 07:18:21|  405|\n",
      "+------------+---------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: Inner Join\n",
    "dfQuestionsSubset\n",
    "    .join(dfTags,Seq(\"id\"),\"inner\")\n",
    "    .select(\"owner_userid\",\"tag\",\"creation_date\",\"score\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------------+-----+\n",
      "|owner_userid|      tag|      creation_date|score|\n",
      "+------------+---------+-------------------+-----+\n",
      "|         131|   xdebug|2008-08-04 07:18:21|  405|\n",
      "|         131| phpstorm|2008-08-04 07:18:21|  405|\n",
      "|         131|debugging|2008-08-04 07:18:21|  405|\n",
      "|         131|  eclipse|2008-08-04 07:18:21|  405|\n",
      "|         131|      php|2008-08-04 07:18:21|  405|\n",
      "+------------+---------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: Left Outer Join\n",
    "dfQuestionsSubset\n",
    "    .join(dfTags, Seq(\"id\"), \"left_outer\")\n",
    "    .select(\"owner_userid\",\"tag\",\"creation_date\",\"score\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|        tag|\n",
      "+-----------+\n",
      "|type-safety|\n",
      "|    jbutton|\n",
      "|     iframe|\n",
      "|  svn-hooks|\n",
      "|  standards|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Query: Distinct\n",
    "dfTags.select(\"tag\").distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Register temp table from dataframe\n",
    "dfTags.createOrReplaceTempView(\"so_tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkSession: org.apache.spark.sql.SparkSession = <lazy>\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// trait Context {\n",
    "\n",
    "//   lazy val sparkConf = new SparkConf()\n",
    "//     .setAppName(\"Learn Spark\")\n",
    "//     .setMaster(\"local[*]\")\n",
    "//     .set(\"spark.cores.max\", \"2\")\n",
    "\n",
    "//   lazy val sparkSession = SparkSession\n",
    "//     .builder()\n",
    "//     .config(sparkConf)\n",
    "//     .getOrCreate()\n",
    "// }\n",
    "\n",
    "lazy val sparkSession = SparkSession\n",
    "    .builder()\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+---------+-----------+\n",
      "|   name|database|description|tableType|isTemporary|\n",
      "+-------+--------+-----------+---------+-----------+\n",
      "|so_tags|    null|       null|TEMPORARY|       true|\n",
      "+-------+--------+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// List all tables in Spark's catalog\n",
    "sparkSession.catalog.listTables().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |  so_tags|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// List all tables in Spark's catalog using Spark SQL\n",
    "sparkSession.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|                tag|\n",
      "+---+-------------------+\n",
      "|  1|               data|\n",
      "|  4|                 c#|\n",
      "|  4|           winforms|\n",
      "|  4|    type-conversion|\n",
      "|  4|            decimal|\n",
      "|  4|            opacity|\n",
      "|  6|               html|\n",
      "|  6|                css|\n",
      "|  6|               css3|\n",
      "|  6|internet-explorer-7|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Select columns\n",
    "sparkSession\n",
    "    .sql(\"select id,tag from so_tags limit 10\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|tag|\n",
      "+---+---+\n",
      "| 23|php|\n",
      "| 42|php|\n",
      "| 85|php|\n",
      "|126|php|\n",
      "|146|php|\n",
      "+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Filter by column value\n",
    "sparkSession.sql(\"select * from so_tags where tag = 'php'\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|php_count|\n",
      "+---------+\n",
      "|      133|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Count number of rows\n",
    "sparkSession.sql(\"\"\"\n",
    "    select count(*) as php_count from so_tags\n",
    "        where tag = 'php'\n",
    "                 \"\"\".stripMargin).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id|          tag|\n",
      "+---+-------------+\n",
      "| 25|      sockets|\n",
      "| 36|          sql|\n",
      "| 36|   sql-server|\n",
      "| 40| structuremap|\n",
      "| 48|submit-button|\n",
      "| 79|          svn|\n",
      "| 79|    subclipse|\n",
      "| 85|          sql|\n",
      "| 90|          svn|\n",
      "|108|          svn|\n",
      "+---+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// SQL like\n",
    "sparkSession.sql(\n",
    "    \"\"\"\n",
    "    select * from so_tags\n",
    "    where tag like 's%'\n",
    "    \"\"\".stripMargin\n",
    "    ).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|    tag|\n",
      "+---+-------+\n",
      "| 25|sockets|\n",
      "|108|    svn|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// SQL where with and clause\n",
    "sparkSession.sql(\n",
    "    \"\"\"\n",
    "    select * from so_tags\n",
    "    where tag like 's%'\n",
    "    and (id = 25 or id = 108)\n",
    "    \"\"\".stripMargin\n",
    "    ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|      tag|\n",
      "+---+---------+\n",
      "| 25|      c++|\n",
      "| 25|        c|\n",
      "| 25|  sockets|\n",
      "| 25|mainframe|\n",
      "| 25|      zos|\n",
      "|108|  windows|\n",
      "|108|      svn|\n",
      "|108|    64bit|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// SQL IN clause\n",
    "sparkSession.sql(\n",
    "    \"\"\"\n",
    "    select * from so_tags\n",
    "    where id in (25,108)\n",
    "    \"\"\".stripMargin\n",
    "    ).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|        tag|count|\n",
      "+-----------+-----+\n",
      "|type-safety|    4|\n",
      "|    jbutton|    1|\n",
      "|     iframe|    2|\n",
      "|  svn-hooks|    2|\n",
      "|  standards|    7|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// SQL Group By\n",
    "sparkSession.sql(\n",
    "    \"\"\"\n",
    "    select tag,count(*) as count\n",
    "        from so_tags\n",
    "        group by tag\n",
    "    \"\"\".stripMargin\n",
    "    ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|          tag|count|\n",
      "+-------------+-----+\n",
      "|    standards|    7|\n",
      "|     keyboard|    8|\n",
      "|          rss|   12|\n",
      "|documentation|   15|\n",
      "|      session|    6|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// SQL Group By with having clause\n",
    "sparkSession.sql(\n",
    "    \"\"\"\n",
    "    select tag,count(*) as count\n",
    "    from so_tags \n",
    "    group by tag\n",
    "    having count > 5\n",
    "    \"\"\".stripMargin\n",
    "    ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|          tag|count|\n",
      "+-------------+-----+\n",
      "|stackoverflow|  382|\n",
      "|           c#|  375|\n",
      "|         .net|  351|\n",
      "|      asp.net|  185|\n",
      "|   sql-server|  167|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// SQL Order by\n",
    "sparkSession.sql(\n",
    "    \"\"\"\n",
    "    select tag, count(*) as count\n",
    "    from so_tags\n",
    "    group by tag\n",
    "    having count >5\n",
    "    order by count desc\n",
    "    \"\"\".stripMargin\n",
    "    ).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// Typed columns, filter and create temp table\n",
    "dfQuestionsSubset.createOrReplaceTempView(\"so_questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "| id|      tag| id|      creation_date|        closed_date|deletion_date|score|owner_userid|answer_count|\n",
      "+---+---------+---+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "|888|   xdebug|888|2008-08-04 07:18:21|2016-08-04 17:22:00|         null|  405|         131|          30|\n",
      "|888| phpstorm|888|2008-08-04 07:18:21|2016-08-04 17:22:00|         null|  405|         131|          30|\n",
      "|888|debugging|888|2008-08-04 07:18:21|2016-08-04 17:22:00|         null|  405|         131|          30|\n",
      "|888|  eclipse|888|2008-08-04 07:18:21|2016-08-04 17:22:00|         null|  405|         131|          30|\n",
      "|888|      php|888|2008-08-04 07:18:21|2016-08-04 17:22:00|         null|  405|         131|          30|\n",
      "+---+---------+---+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// SQL Inner Join\n",
    "// SQL Left Outer Join\n",
    "// SQL Right Outer Join\n",
    "sparkSession.sql(\n",
    "    \"\"\"\n",
    "    select t.*,q.*\n",
    "    from so_questions as q\n",
    "    inner join so_tags t\n",
    "    on t.id = q.id\n",
    "    \"\"\".stripMargin\n",
    "    ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 tag|\n",
      "+--------------------+\n",
      "|         type-safety|\n",
      "|             jbutton|\n",
      "|              iframe|\n",
      "|           svn-hooks|\n",
      "|           standards|\n",
      "|knowledge-management|\n",
      "|            trayicon|\n",
      "|           arguments|\n",
      "|                 zfs|\n",
      "|              import|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// SQL Distinct\n",
    "sparkSession\n",
    "    .sql(\n",
    "        \"\"\"\n",
    "        select distinct tag from so_tags\n",
    "        \"\"\".stripMargin\n",
    "    ).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prefixStackoverflow: (s: String)String\n",
       "res51: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// **Register User Defined Function (UDF)**\n",
    "def prefixStackoverflow(s:String):String = s\"so_$s\"\n",
    "\n",
    "sparkSession.udf\n",
    "    .register(\"prefix_so\",prefixStackoverflow _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|UDF:prefix_so(tag)|\n",
      "+---+------------------+\n",
      "|  1|           so_data|\n",
      "|  4|             so_c#|\n",
      "|  4|       so_winforms|\n",
      "|  4|so_type-conversion|\n",
      "|  4|        so_decimal|\n",
      "+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.sql(\n",
    "    \"\"\"\n",
    "    select id, prefix_so(tag) from so_tags\n",
    "    \"\"\".stripMargin\n",
    "    ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "| id|      creation_date|        closed_date|deletion_date|score|owner_userid|answer_count|\n",
      "+---+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "|  1|2008-08-01 05:26:37|               null|   2011-03-28|    1|        null|           0|\n",
      "|  4|2008-08-01 05:42:52|               null|         null|  472|           8|          13|\n",
      "|  6|2008-08-01 06:08:08|               null|         null|  210|           9|           5|\n",
      "|  8|2008-08-01 07:33:19|2013-06-03 12:00:25|   2015-02-11|   42|        null|           8|\n",
      "+---+-------------------+-------------------+-------------+-----+------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// DataFrame Statistics Introduction\n",
    "dfQuestions.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(score)|\n",
      "+-----------------+\n",
      "|36.14631463146315|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Average\n",
    "import org.apache.spark.sql.functions._\n",
    "dfQuestions.select(avg(\"score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(score)|\n",
      "+----------+\n",
      "|      4443|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Max\n",
    "dfQuestions.select(max(\"score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|min(score)|\n",
      "+----------+\n",
      "|       -27|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Minimum\n",
    "dfQuestions.select(min(\"score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(score)|\n",
      "+-----------------+\n",
      "|36.14631463146315|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Mean\n",
    "dfQuestions.select(mean(\"score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(score)|\n",
      "+----------+\n",
      "|    361427|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//sum\n",
    "dfQuestions.select(sum(\"score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----------------+\n",
      "|owner_userid|avg(score)|max(answer_count)|\n",
      "+------------+----------+-----------------+\n",
      "|         268|      26.0|                1|\n",
      "|         136|      57.6|                9|\n",
      "|         123|      20.0|                3|\n",
      "+------------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Group by with statistics\n",
    "dfQuestions.filter(\"id > 400 and id < 450\")\n",
    "    .filter(\"owner_userid is not null\")\n",
    "    .join(dfTags,dfQuestions.col(\"id\").equalTo(dfTags.col(\"id\")))\n",
    "    .groupBy(dfQuestions.col(\"owner_userid\"))\n",
    "    .agg(avg(\"score\"),max(\"answer_count\"))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+------------------+\n",
      "|summary|               id|             score|     owner_userid|      answer_count|\n",
      "+-------+-----------------+------------------+-----------------+------------------+\n",
      "|  count|             9999|              9999|             7388|              9922|\n",
      "|   mean|33929.17081708171| 36.14631463146315|47389.99472116947|6.6232614392259626|\n",
      "| stddev|19110.09560532429|160.48316753972045|280943.1070344427| 9.069109116851138|\n",
      "|    min|                1|               -27|                1|                -5|\n",
      "|    max|            66037|              4443|          3431280|               316|\n",
      "+-------+-----------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfSummary: org.apache.spark.sql.DataFrame = [summary: string, id: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DataFrame Statistics using describe() method\n",
    "val dfSummary = dfQuestions.describe()\n",
    "dfSummary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation between column score and answer_count = 0.3699847903294707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "correlation: Double = 0.3699847903294707\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Correlation\n",
    "val correlation = dfQuestions.stat.corr(\"score\",\"answer_count\")\n",
    "println(s\"correlation between column score and answer_count = $correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance between column score and answer_count = 537.513381444165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "covariance: Double = 537.513381444165\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Covariance\n",
    "val covariance = dfQuestions.stat.cov(\"score\",\"answer_count\")\n",
    "println(s\"covariance between column score and answer_count = $covariance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|answer_count_freqItems|\n",
      "+----------------------+\n",
      "|  [23, 131, 77, 86,...|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfFrequentScore: org.apache.spark.sql.DataFrame = [answer_count_freqItems: array<int>]\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Frequent Items\n",
    "val dfFrequentScore = dfQuestions.stat.freqItems(Seq(\"answer_count\"))\n",
    "dfFrequentScore.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+---+---+---+---+---+---+---+\n",
      "|score_owner_userid|  1| 11| 13| 17|  2|  3|  4|  5|  8|  9|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+\n",
      "|                56|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
      "|               472|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|\n",
      "|                14|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|\n",
      "|                20|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|\n",
      "|               179|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfScoreByUserid: org.apache.spark.sql.DataFrame = [score_owner_userid: string, 1: bigint ... 9 more fields]\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Crosstab\n",
    "val dfScoreByUserid = dfQuestions\n",
    "    .filter(\"owner_userid > 0 and owner_userid < 20\")\n",
    "    .stat\n",
    "    .crosstab(\"score\",\"owner_userid\")\n",
    "dfScoreByUserid.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|answer_count|count|\n",
      "+------------+-----+\n",
      "|          20|   34|\n",
      "|           5|  811|\n",
      "|          10|  272|\n",
      "+------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfQuestionsByAnswerCount: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, creation_date: timestamp ... 5 more fields]\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Stratified sampling using sampleBy\n",
    "val dfQuestionsByAnswerCount = dfQuestions\n",
    "    .filter(\"owner_userid > 0\")\n",
    "    .filter(\"answer_count in (5,10,20)\")\n",
    "\n",
    "dfQuestionsByAnswerCount.groupBy(\"answer_count\")\n",
    "    .count()\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|answer_count|count|\n",
      "+------------+-----+\n",
      "|          20|   34|\n",
      "|           5|  400|\n",
      "|          10|   26|\n",
      "+------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fractionKeyMap: scala.collection.immutable.Map[Int,Double] = Map(5 -> 0.5, 10 -> 0.1, 20 -> 1.0)\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a fraction map where we are only interested:\n",
    "// - 50% of the rows that have answer_count = 5\n",
    "// - 10% of the rows that have answer_count = 10\n",
    "// - 100% of the rows that have answer_count = 20\n",
    "// Note also that fractions should be in the range [0, 1]\n",
    "\n",
    "val fractionKeyMap = Map(5->0.5, 10->0.1,20->1.0)\n",
    "\n",
    "dfQuestionsByAnswerCount\n",
    "    .stat\n",
    "    .sampleBy(\"answer_count\",fractionKeyMap,7L)\n",
    "    .groupBy(\"answer_count\")\n",
    "    .count().show()\n",
    "\n",
    "//randomseed == 7L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantitles segments = WrappedArray(-27.0, 2.0, 4443.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "quantiles: Array[Double] = Array(-27.0, 2.0, 4443.0)\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Approximate Quantile\n",
    "val quantiles = dfQuestions\n",
    "    .stat\n",
    "    .approxQuantile(\"score\",Array(0,0.5,1),0.25)\n",
    "println(s\"Quantitles segments = ${quantiles.toSeq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tagsBloomFilter: org.apache.spark.util.sketch.BloomFilter = org.apache.spark.util.sketch.BloomFilterImpl@809c4023\n"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Bloom Filter\n",
    "val tagsBloomFilter = dfTags.stat.bloomFilter(\"tag\",1000L,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloom filter contains java tag = true\n"
     ]
    }
   ],
   "source": [
    "println(s\"bloom filter contains java tag = ${tagsBloomFilter.mightContain(\"java\")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloom filter contains some unknown tag = false\n"
     ]
    }
   ],
   "source": [
    "println(s\"bloom filter contains some unknown tag = ${tagsBloomFilter.mightContain(\"unknown tag\")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated frequency for tag java = 513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cmsTag: org.apache.spark.util.sketch.CountMinSketch = org.apache.spark.util.sketch.CountMinSketchImpl@431a88ed\n",
       "estimatedFrequency: Long = 513\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Count Min Sketch\n",
    "    // first parameter = the tag column of dataframe dfTags\n",
    "    // second parameter = 10% precision error factor\n",
    "    // third parameter = 90% confidence level\n",
    "    // fourth parameter = 37 as a random seed\n",
    "\n",
    "val cmsTag = dfTags.stat.countMinSketch(\"tag\", 0.1, 0.9, 37)\n",
    "val estimatedFrequency = cmsTag.estimateCount(\"java\")\n",
    "println(s\"Estimated frequency for tag java = $estimatedFrequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in sample dfTagsSample = 1948\n",
      "Number of rows in dfTags = 9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfTagsSample: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, tag: string]\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Sampling With Replacement\n",
    "    // with replacement = true\n",
    "    // number of rows to sample = 20%\n",
    "    // a random seed = 37L\n",
    "val dfTagsSample = dfTags.sample(true,0.2,37L)\n",
    "println(s\"Number of rows in sample dfTagsSample = ${dfTagsSample.count()}\")\n",
    "println(s\"Number of rows in dfTags = ${dfTags.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrame Operations Introduction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfQuestions: org.apache.spark.sql.DataFrame = [owner_userid: string, tag: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfQuestions = dfQuestionCSV\n",
    "    .filter(\"score > 400 and score < 410\")\n",
    "    .join(dfTags,\"id\")\n",
    "    .select(\"owner_userid\",\"tag\",\"creation_date\",\"score\")\n",
    "    .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dfTags.show(2);dfQuestionCSV.show(2);dfQuestions.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Tag\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Convert DataFrame row to Scala case class\n",
    "case class Tag(id: Int,tag: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id| tag|\n",
      "+---+----+\n",
      "|  1|data|\n",
      "|  4|  c#|\n",
      "+---+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTags.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.sql.Dataset\n",
       "dfTagsOfTag: org.apache.spark.sql.Dataset[Tag] = [id: int, tag: string]\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.Dataset\n",
    "val dfTagsOfTag: Dataset[Tag] = dfTags.as[Tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id = 1, tag = data\n",
      "id = 4, tag = c#\n",
      "id = 4, tag = winforms\n",
      "id = 4, tag = type-conversion\n",
      "id = 4, tag = decimal\n",
      "id = 4, tag = opacity\n",
      "id = 6, tag = html\n",
      "id = 6, tag = css\n",
      "id = 6, tag = css3\n",
      "id = 6, tag = internet-explorer-7\n"
     ]
    }
   ],
   "source": [
    "dfTagsOfTag\n",
    "    .take(10)\n",
    "    .foreach(t => println(s\"id = ${t.id}, tag = ${t.tag}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Question\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DataFrame row to Scala case class using map()\n",
    "case class Question(owner_userid: Int, tag: String,\n",
    "                   creationDate: java.sql.Timestamp, score: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------------------+-----+\n",
      "|owner_userid|     tag|      creation_date|score|\n",
      "+------------+--------+-------------------+-----+\n",
      "|         131|  xdebug|2008-08-04 07:18:21|  405|\n",
      "|         131|phpstorm|2008-08-04 07:18:21|  405|\n",
      "+------------+--------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfQuestions.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toQuestion: (row: org.apache.spark.sql.Row)Question\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def toQuestion(row: org.apache.spark.sql.Row): Question = {\n",
    "// to normalize our owner_userid data\n",
    "    val IntOf: String => Option[Int] = _ match {\n",
    "  case s if s == \"NA\" => None\n",
    "  case s => Some(s.toInt)\n",
    "    }\n",
    "    \n",
    "    import java.time._\n",
    "    val DateOf: String => java.sql.Timestamp = _ match {\n",
    "      case s => java.sql.Timestamp.valueOf(ZonedDateTime.parse(s).toLocalDateTime)\n",
    "    }\n",
    "\n",
    "    Question (\n",
    "      owner_userid = IntOf(row.getString(0)).getOrElse(-1),\n",
    "      tag = row.getString(1),\n",
    "      creationDate = DateOf(row.getString(2)),\n",
    "      score = row.getString(3).toInt\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "dfOfQuestion: org.apache.spark.sql.Dataset[Question] = [owner_userid: int, tag: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val dfOfQuestion: Dataset[Question] = dfQuestions.map(row => toQuestion(row))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|      tag|\n",
      "+---+---------+\n",
      "|  1|  so_java|\n",
      "|  1|   so_jsp|\n",
      "|  2|so_erlang|\n",
      "|  3| so_scala|\n",
      "|  4|  so_akka|\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "segTags: Seq[(Int, String)] = List((1,so_java), (1,so_jsp), (2,so_erlang), (3,so_scala), (4,so_akka))\n",
       "import spark.implicits._\n",
       "dfMoreTags: org.apache.spark.sql.DataFrame = [id: int, tag: string]\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create DataFrame from collection\n",
    "val segTags = Seq(\n",
    "    1 -> \"so_java\",\n",
    "    1 -> \"so_jsp\",\n",
    "    2 -> \"so_erlang\",\n",
    "    3 -> \"so_scala\",\n",
    "    4 -> \"so_akka\"\n",
    ")\n",
    "\n",
    "import spark.implicits._\n",
    "val dfMoreTags = segTags.toDF(\"id\",\"tag\")\n",
    "dfMoreTags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|     tag|\n",
      "+---+--------+\n",
      "|  1|    data|\n",
      "|  1| so_java|\n",
      "|  1|  so_jsp|\n",
      "|  3|so_scala|\n",
      "+---+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfUnionOfTags: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, tag: string]\n"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DataFrame Union\n",
    "val dfUnionOfTags = dfTags.union(dfMoreTags)\n",
    "    .filter(\"id in (1,3)\")\n",
    "dfUnionOfTags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|     tag|\n",
      "+---+--------+\n",
      "|  3|so_scala|\n",
      "|  1| so_java|\n",
      "|  1|  so_jsp|\n",
      "+---+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfIntersectionTags: Unit = ()\n"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DataFrame Intersection\n",
    "val dfIntersectionTags = dfMoreTags.intersect(dfUnionOfTags).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "dfSplitColumn: org.apache.spark.sql.DataFrame = [id: int, tag: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Append column to DataFrame using withColumn()\n",
    "import org.apache.spark.sql.functions._\n",
    "val dfSplitColumn = dfMoreTags\n",
    "    .withColumn(\"tmp\",split($\"tag\",\"_\"))\n",
    "    .select(\n",
    "    $\"id\",\n",
    "    $\"tag\",\n",
    "    $\"tmp\".getItem(0).as(\"so_prefix\"),\n",
    "    $\"tmp\".getItem(1).as(\"so_tag\"))\n",
    "    .drop(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+------+\n",
      "| id|      tag|so_prefix|so_tag|\n",
      "+---+---------+---------+------+\n",
      "|  1|  so_java|       so|  java|\n",
      "|  1|   so_jsp|       so|   jsp|\n",
      "|  2|so_erlang|       so|erlang|\n",
      "|  3| so_scala|       so| scala|\n",
      "|  4|  so_akka|       so|  akka|\n",
      "+---+---------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSplitColumn.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "donuts: Seq[(String, Double)] = List((plain donut,1.5), (vanilla donut,2.0), (glazed donut,2.5))\n",
       "df: org.apache.spark.sql.DataFrame = [Donut Name: string, Price: double]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create DataFrame from Tuples\n",
    "val donuts = Seq((\"plain donut\", 1.50), (\"vanilla donut\", 2.0), (\"glazed donut\", 2.50))\n",
    "val df = sparkSession\n",
    " .createDataFrame(donuts)\n",
    " .toDF(\"Donut Name\", \"Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|   Donut Name|Price|\n",
      "+-------------+-----+\n",
      "|  plain donut|  1.5|\n",
      "|vanilla donut|  2.0|\n",
      "| glazed donut|  2.5|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donut Name\n",
      "Price\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "columnNames: Array[String] = Array(Donut Name, Price)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Get DataFrame column names\n",
    "val columnNames: Array[String] = df.columns\n",
    "columnNames.foreach(name => println(s\"$name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame column names = Donut Name,Price\n",
      "DataFrame column data types = StringType,DoubleType\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "columnNames: Array[String] = Array(Donut Name, Price)\n",
       "columnDataTypes: Array[String] = Array(StringType, DoubleType)\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DataFrame column names and types\n",
    "val (columnNames, columnDataTypes) = df.dtypes.unzip\n",
    "println(s\"DataFrame column names = ${columnNames.mkString(\",\")}\")\n",
    "println(s\"DataFrame column data types = ${columnDataTypes.mkString(\",\")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import sparkSession.sqlContext.implicits._\n",
       "tagsDF: org.apache.spark.sql.DataFrame = [stackoverflow: array<struct<tag:struct<author:string,frameworks:array<struct<id:bigint,name:string>>,id:bigint,name:string>>>]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Json into DataFrame using explode()\n",
    "import sparkSession.sqlContext.implicits._\n",
    "val tagsDF = sparkSession\n",
    "    .read\n",
    "    .option(\"multiline\",true)\n",
    "    .option(\"inferSchema\",true)\n",
    "    .json(\"/Users/soda/Downloads/jsonData.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [stackoverflow_tags: struct<tag: struct<author: string, frameworks: array<struct<id:bigint,name:string>> ... 2 more fields>>]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = tagsDF.select(explode($\"stackoverflow\") as \"stackoverflow_tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------+-------------+--------------------+\n",
      "| id|        author|tag_name|frameworks_id|     frameworks_name|\n",
      "+---+--------------+--------+-------------+--------------------+\n",
      "|  1|Martin Odersky|   scala|       [1, 2]|[Play Framework, ...|\n",
      "|  2| James Gosling|    java|       [1, 2]|[Apache Tomcat, S...|\n",
      "+---+--------------+--------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    $\"stackoverflow_tags.tag.id\" as \"id\",\n",
    "    $\"stackoverflow_tags.tag.author\" as \"author\",\n",
    "    $\"stackoverflow_tags.tag.name\" as \"tag_name\",\n",
    "    $\"stackoverflow_tags.tag.frameworks.id\" as \"frameworks_id\",\n",
    "    $\"stackoverflow_tags.tag.frameworks.name\" as \"frameworks_name\"\n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+\n",
      "| Id|   Donut Name|Price|\n",
      "+---+-------------+-----+\n",
      "|111|  plain donut|  1.5|\n",
      "|222|vanilla donut|  2.0|\n",
      "|333| glazed donut|  2.5|\n",
      "+---+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "donuts: Seq[(String, String, Double)] = List((111,plain donut,1.5), (222,vanilla donut,2.0), (333,glazed donut,2.5))\n",
       "dfDonuts: org.apache.spark.sql.DataFrame = [Id: string, Donut Name: string ... 1 more field]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Concatenate DataFrames using join()\n",
    "val donuts = Seq((\"111\",\"plain donut\", 1.50), (\"222\", \"vanilla donut\", 2.0), (\"333\",\"glazed donut\", 2.50))\n",
    "\n",
    "val dfDonuts = sparkSession\n",
    "    .createDataFrame(donuts)\n",
    "    .toDF(\"Id\",\"Donut Name\", \"Price\")\n",
    "\n",
    "dfDonuts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| Id|Inventory|\n",
      "+---+---------+\n",
      "|111|       10|\n",
      "|222|       20|\n",
      "|333|       30|\n",
      "+---+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inventory: Seq[(String, Int)] = List((111,10), (222,20), (333,30))\n",
       "dfInventory: org.apache.spark.sql.DataFrame = [Id: string, Inventory: int]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inventory = Seq((\"111\", 10), (\"222\", 20), (\"333\", 30))\n",
    "val dfInventory = sparkSession\n",
    "      .createDataFrame(inventory)\n",
    "      .toDF(\"Id\", \"Inventory\")\n",
    "\n",
    "dfInventory.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+---------+\n",
      "| Id|   Donut Name|Price|Inventory|\n",
      "+---+-------------+-----+---------+\n",
      "|111|  plain donut|  1.5|       10|\n",
      "|222|vanilla donut|  2.0|       20|\n",
      "|333| glazed donut|  2.5|       30|\n",
      "+---+-------------+-----+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfDonutsInventory: org.apache.spark.sql.DataFrame = [Id: string, Donut Name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfDonutsInventory = dfDonuts.join(dfInventory,Seq(\"Id\"),\"inner\")\n",
    "dfDonutsInventory.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------+-------------+--------------------+\n",
      "| id|        author|tag_name|frameworks_id|     frameworks_name|\n",
      "+---+--------------+--------+-------------+--------------------+\n",
      "|  1|Martin Odersky|   scala|       [1, 2]|[Play Framework, ...|\n",
      "|  2| James Gosling|    java|       [1, 2]|[Apache Tomcat, S...|\n",
      "+---+--------------+--------+-------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: bigint, author: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = tagsDF\n",
    "    .select(explode($\"stackoverflow\") as \"stackoverflow_tags\")\n",
    "    .select(\n",
    "      $\"stackoverflow_tags.tag.id\" as \"id\",\n",
    "      $\"stackoverflow_tags.tag.author\" as \"author\",\n",
    "      $\"stackoverflow_tags.tag.name\" as \"tag_name\",\n",
    "      $\"stackoverflow_tags.tag.frameworks.id\" as \"frameworks_id\",\n",
    "      $\"stackoverflow_tags.tag.frameworks.name\" as \"frameworks_name\"\n",
    "    )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------+-------------+--------------------+\n",
      "| id|        author|tag_name|frameworks_id|     frameworks_name|\n",
      "+---+--------------+--------+-------------+--------------------+\n",
      "|  1|Martin Odersky|   scala|       [1, 2]|[Play Framework, ...|\n",
      "+---+--------------+--------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "    .select(\"*\")\n",
    "    .where(array_contains($\"frameworks_name\",\"Play Framework\"))\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "donuts: Seq[(String, Double)] = List((plain donut,1.5), (vanilla donut,2.0), (glazed donut,2.5))\n",
       "df: org.apache.spark.sql.DataFrame = [Donut Name: string, Price: double]\n",
       "res18: Boolean = true\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Check DataFrame column exists\n",
    "val donuts = Seq((\"plain donut\", 1.50), (\"vanilla donut\", 2.0), (\"glazed donut\", 2.50))\n",
    "val df = sparkSession.createDataFrame(donuts).toDF(\"Donut Name\", \"Price\")\n",
    "\n",
    "df.columns.contains(\"Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|            Name|    Prices|\n",
      "+----------------+----------+\n",
      "|     Plain Donut|[1.5, 2.0]|\n",
      "|   Vanilla Donut|[2.0, 2.5]|\n",
      "|Strawberry Donut|[2.5, 3.5]|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "targets: Seq[(String, Array[Double])] = List((Plain Donut,Array(1.5, 2.0)), (Vanilla Donut,Array(2.0, 2.5)), (Strawberry Donut,Array(2.5, 3.5)))\n",
       "df: org.apache.spark.sql.DataFrame = [Name: string, Prices: array<double>]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Split DataFrame Array column\n",
    "val targets = Seq((\"Plain Donut\", Array(1.50, 2.0)), (\"Vanilla Donut\", Array(2.0, 2.50)), (\"Strawberry Donut\", Array(2.50, 3.50)))\n",
    "val df = sparkSession\n",
    "    .createDataFrame(targets)\n",
    "    .toDF(\"Name\", \"Prices\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Prices: array (nullable = true)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+----------+\n",
      "|            Name|Low Price|High Price|\n",
      "+----------------+---------+----------+\n",
      "|     Plain Donut|      1.5|       2.0|\n",
      "|   Vanilla Donut|      2.0|       2.5|\n",
      "|Strawberry Donut|      2.5|       3.5|\n",
      "+----------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    $\"Name\",\n",
    "    $\"Prices\"(0).as(\"Low Price\"),\n",
    "    $\"Prices\"(1).as(\"High Price\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|   Donut Name|Price|\n",
      "+-------------+-----+\n",
      "|  plain donut|  1.5|\n",
      "|vanilla donut|  2.0|\n",
      "| glazed donut|  2.5|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "donuts: Seq[(String, Double)] = List((plain donut,1.5), (vanilla donut,2.0), (glazed donut,2.5))\n",
       "df: org.apache.spark.sql.DataFrame = [Donut Name: string, Price: double]\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Rename DataFrame column\n",
    "val donuts = Seq((\"plain donut\", 1.50), (\"vanilla donut\", 2.0), (\"glazed donut\", 2.50))\n",
    "val df = spark.createDataFrame(donuts).toDF(\"Donut Name\", \"Price\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         Name|Price|\n",
      "+-------------+-----+\n",
      "|  plain donut|  1.5|\n",
      "|vanilla donut|  2.0|\n",
      "| glazed donut|  2.5|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"Donut Name\", \"Name\").show()\n",
    "//注意：将“Donut Name”替换为\"Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-----+-----------+----------+\n",
      "|   Donut Name|Price|Tasty|Correlation|     Stock|\n",
      "+-------------+-----+-----+-----------+----------+\n",
      "|  plain donut|  1.5| true|        1.0|[100, 500]|\n",
      "|vanilla donut|  2.0| true|        1.0|[100, 500]|\n",
      "| glazed donut|  2.5| true|        1.0|[100, 500]|\n",
      "+-------------+-----+-----+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "donuts: Seq[(String, Double)] = List((plain donut,1.5), (vanilla donut,2.0), (glazed donut,2.5))\n",
       "df: org.apache.spark.sql.DataFrame = [Donut Name: string, Price: double]\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create DataFrame constant column\n",
    "val donuts = Seq((\"plain donut\", 1.50), (\"vanilla donut\", 2.0), (\"glazed donut\", 2.50))\n",
    "val df = spark.createDataFrame(donuts).toDF(\"Donut Name\", \"Price\")\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "df.withColumn(\"Tasty\",lit(true))\n",
    "    .withColumn(\"Correlation\",lit(1.0))\n",
    "    .withColumn(\"Stock\",typedLit(Seq(100,500))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "donuts: Seq[(String, Double)] = List((plain donut,1.5), (vanilla donut,2.0), (glazed donut,2.5))\n",
       "df: org.apache.spark.sql.DataFrame = [Donut Name: string, Price: double]\n",
       "import org.apache.spark.sql.functions._\n",
       "import spark.sqlContext.implicits._\n",
       "stockMinMax: String => Seq[Int] = <function1>\n",
       "udfStockMinMax: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(IntegerType,false),Some(List(StringType)))\n",
       "df2: org.apache.spark.sql.DataFrame = [Donut Name: string, Price: double ... 1 more field]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DataFrame new column with User Defined Function (UDF)\n",
    "val donuts = Seq((\"plain donut\", 1.50), \n",
    "                 (\"vanilla donut\", 2.0), \n",
    "                 (\"glazed donut\", 2.50))\n",
    "val df = spark.createDataFrame(donuts).toDF(\"Donut Name\", \"Price\")\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import spark.sqlContext.implicits._\n",
    "\n",
    "val stockMinMax: (String => Seq[Int]) = \n",
    "    (donutName: String) => donutName match{\n",
    "        case \"plain donut\" => Seq(100,500)\n",
    "        case \"vanilla donut\" => Seq(200,400)\n",
    "        case \"glazed donut\" => Seq(200,600)\n",
    "        case _ => Seq(150,150)\n",
    "    }\n",
    "\n",
    "val udfStockMinMax = udf(stockMinMax)\n",
    "\n",
    "val df2 = df.withColumn(\"Stock Min Max\",udfStockMinMax($\"Donut Name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+\n",
      "|   Donut Name|Price|Stock Min Max|\n",
      "+-------------+-----+-------------+\n",
      "|  plain donut|  1.5|   [100, 500]|\n",
      "|vanilla donut|  2.0|   [200, 400]|\n",
      "| glazed donut|  2.5|   [200, 600]|\n",
      "+-------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|   Donut Name|Price|\n",
      "+-------------+-----+\n",
      "|  plain donut|  1.5|\n",
      "|vanilla donut|  2.0|\n",
      "| glazed donut|  2.5|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "donuts: Seq[(String, Double)] = List((plain donut,1.5), (vanilla donut,2.0), (glazed donut,2.5))\n",
       "df: org.apache.spark.sql.DataFrame = [Donut Name: string, Price: double]\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DataFrame First Row\n",
    "val donuts = Seq((\"plain donut\", 1.50), (\"vanilla donut\", 2.0), (\"glazed donut\", 2.50))\n",
    "val df = sparkSession\n",
    "    .createDataFrame(donuts)\n",
    "    .toDF(\"Donut Name\", \"Price\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([plain donut,1.5],plain donut,1.5)\n"
     ]
    }
   ],
   "source": [
    "println(df.first(),\n",
    "        df.first().get(0),\n",
    "        df.first().getAs[Double](\"Price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+---------------+--------------------+--------------+--------------+--------------+---+-----+----+\n",
      "|   Donut Name|Price|Purchase Date|Price Formatted|      Name Formatted|Name Uppercase|Name Lowercase|Date Formatted|Day|Month|Year|\n",
      "+-------------+-----+-------------+---------------+--------------------+--------------+--------------+--------------+---+-----+----+\n",
      "|  plain donut|  1.5|   2018-04-17|           1.50| awesome plain donut|   PLAIN DONUT|   plain donut|      20180417| 17|    4|2018|\n",
      "|vanilla donut|  2.0|   2018-04-01|           2.00|awesome vanilla d...| VANILLA DONUT| vanilla donut|      20180401|  1|    4|2018|\n",
      "| glazed donut|  2.5|   2018-04-02|           2.50|awesome glazed donut|  GLAZED DONUT|  glazed donut|      20180402|  2|    4|2018|\n",
      "+-------------+-----+-------------+---------------+--------------------+--------------+--------------+--------------+---+-----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "donuts: Seq[(String, Double, String)] = List((plain donut,1.5,2018-04-17), (vanilla donut,2.0,2018-04-01), (glazed donut,2.5,2018-04-02))\n",
       "df: org.apache.spark.sql.DataFrame = [Donut Name: string, Price: double ... 1 more field]\n",
       "import org.apache.spark.sql.functions._\n",
       "import spark.sqlContext.implicits._\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Format DataFrame column\n",
    "val donuts = Seq((\"plain donut\", 1.50, \"2018-04-17\"), (\"vanilla donut\", 2.0, \"2018-04-01\"), (\"glazed donut\", 2.50, \"2018-04-02\"))\n",
    "val df = spark.createDataFrame(donuts).toDF(\"Donut Name\", \"Price\", \"Purchase Date\")\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import spark.sqlContext.implicits._\n",
    "\n",
    "df\n",
    ".withColumn(\"Price Formatted\", format_number($\"Price\", 2))\n",
    ".withColumn(\"Name Formatted\", format_string(\"awesome %s\", $\"Donut Name\"))\n",
    ".withColumn(\"Name Uppercase\", upper($\"Donut Name\"))\n",
    ".withColumn(\"Name Lowercase\", lower($\"Donut Name\"))\n",
    ".withColumn(\"Date Formatted\", date_format($\"Purchase Date\", \"yyyyMMdd\"))\n",
    ".withColumn(\"Day\", dayofmonth($\"Purchase Date\"))\n",
    ".withColumn(\"Month\", month($\"Purchase Date\"))\n",
    ".withColumn(\"Year\", year($\"Purchase Date\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+--------------+------+-------------+-------------+-------------+-------------+---------+------+-------------------+-------------+\n",
      "|   Donut Name|Price|Purchase Date|Contains plain|Length|         Trim|        LTrim|        RTrim|      Reverse|Substring|IsNull|             Concat|      InitCap|\n",
      "+-------------+-----+-------------+--------------+------+-------------+-------------+-------------+-------------+---------+------+-------------------+-------------+\n",
      "|  plain donut|  1.5|   2018-04-17|             7|    11|  plain donut|  plain donut|  plain donut|  tunod nialp|    plain| false|  plain donut - 1.5|  Plain Donut|\n",
      "|vanilla donut|  2.0|   2018-04-01|             9|    13|vanilla donut|vanilla donut|vanilla donut|tunod allinav|    vanil| false|vanilla donut - 2.0|Vanilla Donut|\n",
      "| glazed donut|  2.5|   2018-04-02|             8|    12| glazed donut| glazed donut| glazed donut| tunod dezalg|    glaze| false| glazed donut - 2.5| Glazed Donut|\n",
      "+-------------+-----+-------------+--------------+------+-------------+-------------+-------------+-------------+---------+------+-------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "donuts: Seq[(String, Double, String)] = List((plain donut,1.5,2018-04-17), (vanilla donut,2.0,2018-04-01), (glazed donut,2.5,2018-04-02))\n",
       "df: org.apache.spark.sql.DataFrame = [Donut Name: string, Price: double ... 1 more field]\n",
       "import org.apache.spark.sql.functions._\n",
       "import spark.sqlContext.implicits._\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// DataFrame String Functions\n",
    "val donuts = Seq((\"plain donut\", 1.50, \"2018-04-17\"), (\"vanilla donut\", 2.0, \"2018-04-01\"), (\"glazed donut\", 2.50, \"2018-04-02\"))\n",
    "val df = spark\n",
    "\t.createDataFrame(donuts)\n",
    "\t.toDF(\"Donut Name\", \"Price\", \"Purchase Date\")\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import spark.sqlContext.implicits._\n",
    "\n",
    "df\n",
    ".withColumn(\"Contains plain\", instr($\"Donut Name\", \"donut\"))\n",
    ".withColumn(\"Length\", length($\"Donut Name\"))\n",
    ".withColumn(\"Trim\", trim($\"Donut Name\"))\n",
    ".withColumn(\"LTrim\", ltrim($\"Donut Name\"))\n",
    ".withColumn(\"RTrim\", rtrim($\"Donut Name\"))\n",
    ".withColumn(\"Reverse\", reverse($\"Donut Name\"))\n",
    ".withColumn(\"Substring\", substring($\"Donut Name\", 0, 5))\n",
    ".withColumn(\"IsNull\", isnull($\"Donut Name\"))\n",
    ".withColumn(\"Concat\", concat_ws(\" - \", $\"Donut Name\", $\"Price\"))\n",
    ".withColumn(\"InitCap\", initcap($\"Donut Name\"))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// DataFrame drop null\n",
    "dfWithNull.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. http://localhost:8888/notebooks/Downloads/backUp/dataset/GPS/%F0%9F%9A%98RAI%20%E2%9C%A8.ipynb\n",
    "2. http://localhost:8888/notebooks/Downloads/backUp/dataset/GPS/%F0%9F%A6%88GPSpipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
